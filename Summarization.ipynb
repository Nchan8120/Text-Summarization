{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "## This notebook outlines the concepts behind Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "- concept of capturing very important gist of a long piece of text\n",
    "\n",
    "### Types of Summarization\n",
    "- 1. **Extractive Summarization**\n",
    "    - Select sentences from the corpus that best represent the text\n",
    "    - Arrange them to form a summary\n",
    "- 2. **Abstractive Summarization**\n",
    "    - Captures the very important sentences from the text\n",
    "    - Paraphrases them to form a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Libraries\n",
    "- Sumy\n",
    "- Gensim\n",
    "- Summa\n",
    "- BERT **\n",
    "    - BART **\n",
    "    - PEGASUS **\n",
    "    - T5 **\n",
    "\n",
    "** Will be seen in DL-1\n",
    "\n",
    "\n",
    "## 1. Sumy :\n",
    "    1. Luhn – Heurestic method\n",
    "    2. Latent Semantic Analysis\n",
    "    4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
    "    5. TextRank - Graph-based summarization technique with keyword extractions in from document\n",
    "\n",
    "Documentation Reference [sumy](https://github.com/miso-belica/sumy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Take a piece of text from wiki page and summarize them using Sumy\n",
    "### Steps\n",
    "- Install the necessary libraries\n",
    "- Import the libraries\n",
    "- Scrape the text from a pre-defined webpage\n",
    "- Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries\n",
    "- HtmlParser\n",
    "- Tokenizer\n",
    "- TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<title>Automatic summarization - Wikipedia</title>\\n<script>(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split(\\'%2C\\').forEach(function(pref){className=className.replace(new RegExp(\\'(^| )\\'+pref.replace(/-clientpref-\\\\w+$|[^\\\\w-]+/g,\\'\\')+\\'-clientpref-\\\\\\\\w+( |$)\\'),\\'$1\\'+pref+\\'$2\\');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"9cf89ee6-94b9-43fe-9776-2f90c8488f16\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Automatic_summarization\",\"wgTitle\":\"Automatic summarization\",\"wgCurRevisionId\":1236297853,\"wgRevisionId\":1236297853,\"wgArticleId\":637199,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 maint: location missing publisher\",\"CS1 maint: archived copy as title\",\"CS1 maint: bot: original URL status unknown\",\"Webarchive template wayback links\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles needing additional references from April 2022\",\"All articles needing additional references\",\"All accuracy disputes\",\"Articles with disputed statements from June 2018\",\"All articles with unsourced statements\",\"Articles with unsourced statements from June 2018\",\"Articles to be expanded from February 2017\",\"All articles to be expanded\",\"CS1 maint: multiple names: authors list\",\"Computational linguistics\",\"Natural language processing\",\"Tasks of natural language processing\",\"Data mining\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Automatic_summarization\",\"wgRelevantArticleId\":637199,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":50000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q1394144\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};\\nRLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.echo.centralauth\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.interface\",\"ext.cx.eventlogging.campaigns\",\"ext.cx.uls.quick.actions\",\"wikibase.client.vector-2022\",\"ext.checkUser.clientHints\",\"ext.growthExperiments.SuggestedEditSession\"];</script>\\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return[\"user.options@12s5i\",function($,jQuery,require,module){mw.user.tokens.set({\"patrolToken\":\"+\\\\\\\\\",\"watchToken\":\"+\\\\\\\\\",\"csrfToken\":\"+\\\\\\\\\"});\\n}];});});</script>\\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediamessages.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022\">\\n<script async=\"\" src=\"/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022\"></script>\\n<meta name=\"ResourceLoaderDynamicStyles\" content=\"\">\\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022\">\\n<meta name=\"generator\" content=\"MediaWiki 1.44.0-wmf.20\">\\n<meta name=\"referrer\" content=\"origin\">\\n<meta name=\"referrer\" content=\"origin-when-cross-origin\">\\n<meta name=\"robots\" content=\"max-image-preview:standard\">\\n<meta name=\"format-detection\" content=\"telephone=no\">\\n<meta name=\"viewport\" content=\"width=1120\">\\n<meta property=\"og:title\" content=\"Automatic summarization - Wikipedia\">\\n<meta property=\"og:type\" content=\"website\">\\n<link rel=\"preconnect\" href=\"//upload.wikimedia.org\">\\n<link rel=\"alternate\" media=\"only screen and (max-width: 640px)\" href=\"//en.m.wikipedia.org/wiki/Automatic_summarization\">\\n<link rel=\"alternate\" type=\"application/x-wiki\" title=\"Edit this page\" href=\"/w/index.php?title=Automatic_summarization&amp;action=edit\">\\n<link rel=\"apple-touch-icon\" href=\"/static/apple-touch/wikipedia.png\">\\n<link rel=\"icon\" href=\"/static/favicon/wikipedia.ico\">\\n<link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/w/rest.php/v1/search\" title=\"Wikipedia (en)\">\\n<link rel=\"EditURI\" type=\"application/rsd+xml\" href=\"//en.wikipedia.org/w/api.php?action=rsd\">\\n<link rel=\"canonical\" href=\"https://en.wikipedia.org/wiki/Automatic_summarization\">\\n<link rel=\"license\" href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.en\">\\n<link rel=\"alternate\" type=\"application/atom+xml\" title=\"Wikipedia Atom feed\" href=\"/w/index.php?title=Special:RecentChanges&amp;feed=atom\">\\n<link rel=\"dns-prefetch\" href=\"//meta.wikimedia.org\" />\\n<link rel=\"dns-prefetch\" href=\"login.wikimedia.org\">\\n</head>\\n<body class=\"skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Automatic_summarization rootpage-Automatic_summarization skin-vector-2022 action-view\"><a class=\"mw-jump-link\" href=\"#bodyContent\">Jump to content</a>\\n<div class=\"vector-header-container\">\\n\\t<header class=\"vector-header mw-header\">\\n\\t\\t<div class=\"vector-header-start\">\\n\\t\\t\\t<nav class=\"vector-main-menu-landmark\" aria-label=\"Site\">\\n\\t\\t\\t\\t\\n<div id=\"vector-main-menu-dropdown\" class=\"vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right\"  title=\"Main menu\" >\\n\\t<input type=\"checkbox\" id=\"vector-main-menu-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-main-menu-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Main menu\"  >\\n\\t<label id=\"vector-main-menu-dropdown-label\" for=\"vector-main-menu-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu\"></span>\\n\\n<span class=\"vector-dropdown-label-text\">Main menu</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\n\\t\\t\\t\\t<div id=\"vector-main-menu-unpinned-container\" class=\"vector-unpinned-container\">\\n\\t\\t\\n<div id=\"vector-main-menu\" class=\"vector-main-menu vector-pinnable-element\">\\n\\t<div\\n\\tclass=\"vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned\"\\n\\tdata-feature-name=\"main-menu-pinned\"\\n\\tdata-pinnable-element-id=\"vector-main-menu\"\\n\\tdata-pinned-container-id=\"vector-main-menu-pinned-container\"\\n\\tdata-unpinned-container-id=\"vector-main-menu-unpinned-container\"\\n>\\n\\t<div class=\"vector-pinnable-header-label\">Main menu</div>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-main-menu.pin\">move to sidebar</button>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-main-menu.unpin\">hide</button>\\n</div>\\n\\n\\t\\n<div id=\"p-navigation\" class=\"vector-menu mw-portlet mw-portlet-navigation\"  >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tNavigation\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"n-mainpage-description\" class=\"mw-list-item\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\"><span>Main page</span></a></li><li id=\"n-contents\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:Contents\" title=\"Guides to browsing Wikipedia\"><span>Contents</span></a></li><li id=\"n-currentevents\" class=\"mw-list-item\"><a href=\"/wiki/Portal:Current_events\" title=\"Articles related to current events\"><span>Current events</span></a></li><li id=\"n-randompage\" class=\"mw-list-item\"><a href=\"/wiki/Special:Random\" title=\"Visit a randomly selected article [x]\" accesskey=\"x\"><span>Random article</span></a></li><li id=\"n-aboutsite\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:About\" title=\"Learn about Wikipedia and how it works\"><span>About Wikipedia</span></a></li><li id=\"n-contactpage\" class=\"mw-list-item\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\"><span>Contact us</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\n\\t\\n<div id=\"p-interaction\" class=\"vector-menu mw-portlet mw-portlet-interaction\"  >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tContribute\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"n-help\" class=\"mw-list-item\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\"><span>Help</span></a></li><li id=\"n-introduction\" class=\"mw-list-item\"><a href=\"/wiki/Help:Introduction\" title=\"Learn how to edit Wikipedia\"><span>Learn to edit</span></a></li><li id=\"n-portal\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"The hub for editors\"><span>Community portal</span></a></li><li id=\"n-recentchanges\" class=\"mw-list-item\"><a href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes to Wikipedia [r]\" accesskey=\"r\"><span>Recent changes</span></a></li><li id=\"n-upload\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:File_upload_wizard\" title=\"Add images or other media for use on Wikipedia\"><span>Upload file</span></a></li><li id=\"n-specialpages\" class=\"mw-list-item\"><a href=\"/wiki/Special:SpecialPages\"><span>Special pages</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n</div>\\n\\n\\t\\t\\t\\t</div>\\n\\n\\t</div>\\n</div>\\n\\n\\t\\t</nav>\\n\\t\\t\\t\\n<a href=\"/wiki/Main_Page\" class=\"mw-logo\">\\n\\t<img class=\"mw-logo-icon\" src=\"/static/images/icons/wikipedia.png\" alt=\"\" aria-hidden=\"true\" height=\"50\" width=\"50\">\\n\\t<span class=\"mw-logo-container skin-invert\">\\n\\t\\t<img class=\"mw-logo-wordmark\" alt=\"Wikipedia\" src=\"/static/images/mobile/copyright/wikipedia-wordmark-en.svg\" style=\"width: 7.5em; height: 1.125em;\">\\n\\t\\t<img class=\"mw-logo-tagline\" alt=\"The Free Encyclopedia\" src=\"/static/images/mobile/copyright/wikipedia-tagline-en.svg\" width=\"117\" height=\"13\" style=\"width: 7.3125em; height: 0.8125em;\">\\n\\t</span>\\n</a>\\n\\n\\t\\t</div>\\n\\t\\t<div class=\"vector-header-end\">\\n\\t\\t\\t\\n<div id=\"p-search\" role=\"search\" class=\"vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box\">\\n\\t<a href=\"/wiki/Special:Search\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle\" title=\"Search Wikipedia [f]\" accesskey=\"f\"><span class=\"vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search\"></span>\\n\\n<span>Search</span>\\n\\t</a>\\n\\t<div class=\"vector-typeahead-search-container\">\\n\\t\\t<div class=\"cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width\">\\n\\t\\t\\t<form action=\"/w/index.php\" id=\"searchform\" class=\"cdx-search-input cdx-search-input--has-end-button\">\\n\\t\\t\\t\\t<div id=\"simpleSearch\" class=\"cdx-search-input__input-wrapper\"  data-search-loc=\"header-moved\">\\n\\t\\t\\t\\t\\t<div class=\"cdx-text-input cdx-text-input--has-start-icon\">\\n\\t\\t\\t\\t\\t\\t<input\\n\\t\\t\\t\\t\\t\\t\\tclass=\"cdx-text-input__input\"\\n\\t\\t\\t\\t\\t\\t\\t type=\"search\" name=\"search\" placeholder=\"Search Wikipedia\" aria-label=\"Search Wikipedia\" autocapitalize=\"sentences\" title=\"Search Wikipedia [f]\" accesskey=\"f\" id=\"searchInput\"\\n\\t\\t\\t\\t\\t\\t\\t>\\n\\t\\t\\t\\t\\t\\t<span class=\"cdx-text-input__icon cdx-text-input__start-icon\"></span>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t<input type=\"hidden\" name=\"title\" value=\"Special:Search\">\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t<button class=\"cdx-button cdx-search-input__end-button\">Search</button>\\n\\t\\t\\t</form>\\n\\t\\t</div>\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t<nav class=\"vector-user-links vector-user-links-wide\" aria-label=\"Personal tools\">\\n\\t<div class=\"vector-user-links-main\">\\n\\t\\n<div id=\"p-vector-user-menu-preferences\" class=\"vector-menu mw-portlet emptyPortlet\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\n<div id=\"p-vector-user-menu-userpage\" class=\"vector-menu mw-portlet emptyPortlet\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t<nav class=\"vector-appearance-landmark\" aria-label=\"Appearance\">\\n\\t\\t\\n<div id=\"vector-appearance-dropdown\" class=\"vector-dropdown \"  title=\"Change the appearance of the page&#039;s font size, width, and color\" >\\n\\t<input type=\"checkbox\" id=\"vector-appearance-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-appearance-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Appearance\"  >\\n\\t<label id=\"vector-appearance-dropdown-label\" for=\"vector-appearance-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance\"></span>\\n\\n<span class=\"vector-dropdown-label-text\">Appearance</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\n\\t\\t\\t<div id=\"vector-appearance-unpinned-container\" class=\"vector-unpinned-container\">\\n\\t\\t\\t\\t\\n\\t\\t\\t</div>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t</nav>\\n\\t\\n<div id=\"p-vector-user-menu-notifications\" class=\"vector-menu mw-portlet emptyPortlet\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\n<div id=\"p-vector-user-menu-overflow\" class=\"vector-menu mw-portlet\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t<li id=\"pt-sitesupport-2\" class=\"user-links-collapsible-item mw-list-item user-links-collapsible-item\"><a data-mw=\"interface\" href=\"https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en\" class=\"\"><span>Donate</span></a>\\n</li>\\n<li id=\"pt-createaccount-2\" class=\"user-links-collapsible-item mw-list-item user-links-collapsible-item\"><a data-mw=\"interface\" href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Automatic+summarization\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\" class=\"\"><span>Create account</span></a>\\n</li>\\n<li id=\"pt-login-2\" class=\"user-links-collapsible-item mw-list-item user-links-collapsible-item\"><a data-mw=\"interface\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Automatic+summarization\" title=\"You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]\" accesskey=\"o\" class=\"\"><span>Log in</span></a>\\n</li>\\n\\n\\t\\t\\t\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t</div>\\n\\t\\n<div id=\"vector-user-links-dropdown\" class=\"vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out\"  title=\"Log in and more options\" >\\n\\t<input type=\"checkbox\" id=\"vector-user-links-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-user-links-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Personal tools\"  >\\n\\t<label id=\"vector-user-links-dropdown-label\" for=\"vector-user-links-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis\"></span>\\n\\n<span class=\"vector-dropdown-label-text\">Personal tools</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\n\\t\\t\\n<div id=\"p-personal\" class=\"vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item\"  title=\"User menu\" >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"pt-sitesupport\" class=\"user-links-collapsible-item mw-list-item\"><a href=\"https://donate.wikimedia.org/?wmf_source=donate&amp;wmf_medium=sidebar&amp;wmf_campaign=en.wikipedia.org&amp;uselang=en\"><span>Donate</span></a></li><li id=\"pt-createaccount\" class=\"user-links-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Automatic+summarization\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\"><span class=\"vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd\"></span> <span>Create account</span></a></li><li id=\"pt-login\" class=\"user-links-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Automatic+summarization\" title=\"You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]\" accesskey=\"o\"><span class=\"vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn\"></span> <span>Log in</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n<div id=\"p-user-menu-anon-editor\" class=\"vector-menu mw-portlet mw-portlet-user-menu-anon-editor\"  >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tPages for logged out editors <a href=\"/wiki/Help:Introduction\" aria-label=\"Learn more about editing\"><span>learn more</span></a>\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"pt-anoncontribs\" class=\"mw-list-item\"><a href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\" accesskey=\"y\"><span>Contributions</span></a></li><li id=\"pt-anontalk\" class=\"mw-list-item\"><a href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\" accesskey=\"n\"><span>Talk</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\n\\t</div>\\n</div>\\n\\n</nav>\\n\\n\\t\\t</div>\\n\\t</header>\\n</div>\\n<div class=\"mw-page-container\">\\n\\t<div class=\"mw-page-container-inner\">\\n\\t\\t<div class=\"vector-sitenotice-container\">\\n\\t\\t\\t<div id=\"siteNotice\"><!-- CentralNotice --></div>\\n\\t\\t</div>\\n\\t\\t<div class=\"vector-column-start\">\\n\\t\\t\\t<div class=\"vector-main-menu-container\">\\n\\t\\t<div id=\"mw-navigation\">\\n\\t\\t\\t<nav id=\"mw-panel\" class=\"vector-main-menu-landmark\" aria-label=\"Site\">\\n\\t\\t\\t\\t<div id=\"vector-main-menu-pinned-container\" class=\"vector-pinned-container\">\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t</div>\\n\\t\\t</nav>\\n\\t\\t</div>\\n\\t</div>\\n\\t<div class=\"vector-sticky-pinned-container\">\\n\\t\\t\\t\\t<nav id=\"mw-panel-toc\" aria-label=\"Contents\" data-event-name=\"ui.sidebar-toc\" class=\"mw-table-of-contents-container vector-toc-landmark\">\\n\\t\\t\\t\\t\\t<div id=\"vector-toc-pinned-container\" class=\"vector-pinned-container\">\\n\\t\\t\\t\\t\\t<div id=\"vector-toc\" class=\"vector-toc vector-pinnable-element\">\\n\\t<div\\n\\tclass=\"vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned\"\\n\\tdata-feature-name=\"toc-pinned\"\\n\\tdata-pinnable-element-id=\"vector-toc\"\\n\\t\\n\\t\\n>\\n\\t<h2 class=\"vector-pinnable-header-label\">Contents</h2>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-toc.pin\">move to sidebar</button>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-toc.unpin\">hide</button>\\n</div>\\n\\n\\n\\t<ul class=\"vector-toc-contents\" id=\"mw-panel-toc-list\">\\n\\t\\t<li id=\"toc-mw-content-text\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t\\t<a href=\"#\" class=\"vector-toc-link\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">(Top)</div>\\n\\t\\t\\t</a>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Commercial_products\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#Commercial_products\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">1</span>\\n\\t\\t\\t\\t<span>Commercial products</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t<ul id=\"toc-Commercial_products-sublist\" class=\"vector-toc-list\">\\n\\t\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-Approaches\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#Approaches\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">2</span>\\n\\t\\t\\t\\t<span>Approaches</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<button aria-controls=\"toc-Approaches-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\\n\\t\\t\\t\\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\\n\\t\\t\\t\\t<span>Toggle Approaches subsection</span>\\n\\t\\t\\t</button>\\n\\t\\t\\n\\t\\t<ul id=\"toc-Approaches-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t<li id=\"toc-Extraction-based_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Extraction-based_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">2.1</span>\\n\\t\\t\\t\\t\\t<span>Extraction-based summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Extraction-based_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Abstractive-based_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Abstractive-based_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">2.2</span>\\n\\t\\t\\t\\t\\t<span>Abstractive-based summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Abstractive-based_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Aided_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Aided_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">2.3</span>\\n\\t\\t\\t\\t\\t<span>Aided summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Aided_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-Applications_and_systems_for_summarization\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#Applications_and_systems_for_summarization\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">3</span>\\n\\t\\t\\t\\t<span>Applications and systems for summarization</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<button aria-controls=\"toc-Applications_and_systems_for_summarization-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\\n\\t\\t\\t\\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\\n\\t\\t\\t\\t<span>Toggle Applications and systems for summarization subsection</span>\\n\\t\\t\\t</button>\\n\\t\\t\\n\\t\\t<ul id=\"toc-Applications_and_systems_for_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t<li id=\"toc-Keyphrase_extraction\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Keyphrase_extraction\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.1</span>\\n\\t\\t\\t\\t\\t<span>Keyphrase extraction</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Keyphrase_extraction-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t\\t<li id=\"toc-Supervised_learning_approaches\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Supervised_learning_approaches\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.1.1</span>\\n\\t\\t\\t\\t\\t<span>Supervised learning approaches</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Supervised_learning_approaches-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Unsupervised_approach:_TextRank\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Unsupervised_approach:_TextRank\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.1.2</span>\\n\\t\\t\\t\\t\\t<span>Unsupervised approach: TextRank</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Unsupervised_approach:_TextRank-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Document_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Document_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2</span>\\n\\t\\t\\t\\t\\t<span>Document summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Document_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t\\t<li id=\"toc-Supervised_learning_approaches_2\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Supervised_learning_approaches_2\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2.1</span>\\n\\t\\t\\t\\t\\t<span>Supervised learning approaches</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Supervised_learning_approaches_2-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Maximum_entropy-based_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Maximum_entropy-based_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2.2</span>\\n\\t\\t\\t\\t\\t<span>Maximum entropy-based summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Maximum_entropy-based_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Adaptive_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Adaptive_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2.3</span>\\n\\t\\t\\t\\t\\t<span>Adaptive summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Adaptive_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-TextRank_and_LexRank\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#TextRank_and_LexRank\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2.4</span>\\n\\t\\t\\t\\t\\t<span>TextRank and LexRank</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-TextRank_and_LexRank-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Multi-document_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-3\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Multi-document_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2.5</span>\\n\\t\\t\\t\\t\\t<span>Multi-document summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Multi-document_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t\\t<li id=\"toc-Diversity\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-4\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Diversity\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.2.5.1</span>\\n\\t\\t\\t\\t\\t<span>Diversity</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Diversity-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Submodular_functions_as_generic_tools_for_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Submodular_functions_as_generic_tools_for_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.3</span>\\n\\t\\t\\t\\t\\t<span>Submodular functions as generic tools for summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Submodular_functions_as_generic_tools_for_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Applications\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Applications\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">3.4</span>\\n\\t\\t\\t\\t\\t<span>Applications</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Applications-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-Evaluation\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#Evaluation\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">4</span>\\n\\t\\t\\t\\t<span>Evaluation</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<button aria-controls=\"toc-Evaluation-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\\n\\t\\t\\t\\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\\n\\t\\t\\t\\t<span>Toggle Evaluation subsection</span>\\n\\t\\t\\t</button>\\n\\t\\t\\n\\t\\t<ul id=\"toc-Evaluation-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t<li id=\"toc-Intrinsic_versus_extrinsic\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Intrinsic_versus_extrinsic\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">4.1</span>\\n\\t\\t\\t\\t\\t<span>Intrinsic versus extrinsic</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Intrinsic_versus_extrinsic-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Inter-textual_versus_intra-textual\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Inter-textual_versus_intra-textual\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">4.2</span>\\n\\t\\t\\t\\t\\t<span>Inter-textual versus intra-textual</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Inter-textual_versus_intra-textual-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Domain-specific_versus_domain-independent_summarization\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Domain-specific_versus_domain-independent_summarization\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">4.3</span>\\n\\t\\t\\t\\t\\t<span>Domain-specific versus domain-independent summarization</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Domain-specific_versus_domain-independent_summarization-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t\\t<li id=\"toc-Qualitative\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Qualitative\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">4.4</span>\\n\\t\\t\\t\\t\\t<span>Qualitative</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Qualitative-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-History\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#History\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">5</span>\\n\\t\\t\\t\\t<span>History</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<button aria-controls=\"toc-History-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\\n\\t\\t\\t\\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\\n\\t\\t\\t\\t<span>Toggle History subsection</span>\\n\\t\\t\\t</button>\\n\\t\\t\\n\\t\\t<ul id=\"toc-History-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t<li id=\"toc-Recent_approaches\"\\n\\t\\t\\tclass=\"vector-toc-list-item vector-toc-level-2\">\\n\\t\\t\\t<a class=\"vector-toc-link\" href=\"#Recent_approaches\">\\n\\t\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t\\t<span class=\"vector-toc-numb\">5.1</span>\\n\\t\\t\\t\\t\\t<span>Recent approaches</span>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t<ul id=\"toc-Recent_approaches-sublist\" class=\"vector-toc-list\">\\n\\t\\t\\t</ul>\\n\\t\\t</li>\\n\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-See_also\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#See_also\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">6</span>\\n\\t\\t\\t\\t<span>See also</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t<ul id=\"toc-See_also-sublist\" class=\"vector-toc-list\">\\n\\t\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-References\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#References\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">7</span>\\n\\t\\t\\t\\t<span>References</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t<ul id=\"toc-References-sublist\" class=\"vector-toc-list\">\\n\\t\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-Works_cited\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#Works_cited\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">8</span>\\n\\t\\t\\t\\t<span>Works cited</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t<ul id=\"toc-Works_cited-sublist\" class=\"vector-toc-list\">\\n\\t\\t</ul>\\n\\t</li>\\n\\t<li id=\"toc-Further_reading\"\\n\\t\\tclass=\"vector-toc-list-item vector-toc-level-1\">\\n\\t\\t<a class=\"vector-toc-link\" href=\"#Further_reading\">\\n\\t\\t\\t<div class=\"vector-toc-text\">\\n\\t\\t\\t\\t<span class=\"vector-toc-numb\">9</span>\\n\\t\\t\\t\\t<span>Further reading</span>\\n\\t\\t\\t</div>\\n\\t\\t</a>\\n\\t\\t\\n\\t\\t<ul id=\"toc-Further_reading-sublist\" class=\"vector-toc-list\">\\n\\t\\t</ul>\\n\\t</li>\\n</ul>\\n</div>\\n\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t</nav>\\n\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t<div class=\"mw-content-container\">\\n\\t\\t\\t<main id=\"content\" class=\"mw-body\">\\n\\t\\t\\t\\t<header class=\"mw-body-header vector-page-titlebar\">\\n\\t\\t\\t\\t\\t<nav aria-label=\"Contents\" class=\"vector-toc-landmark\">\\n\\t\\t\\t\\t\\t\\t\\n<div id=\"vector-page-titlebar-toc\" class=\"vector-dropdown vector-page-titlebar-toc vector-button-flush-left\"  title=\"Table of Contents\" >\\n\\t<input type=\"checkbox\" id=\"vector-page-titlebar-toc-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-page-titlebar-toc\" class=\"vector-dropdown-checkbox \"  aria-label=\"Toggle the table of contents\"  >\\n\\t<label id=\"vector-page-titlebar-toc-label\" for=\"vector-page-titlebar-toc-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet\"></span>\\n\\n<span class=\"vector-dropdown-label-text\">Toggle the table of contents</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t<div id=\"vector-page-titlebar-toc-unpinned-container\" class=\"vector-unpinned-container\">\\n\\t\\t\\t</div>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t\\t\\t</nav>\\n\\t\\t\\t\\t\\t<h1 id=\"firstHeading\" class=\"firstHeading mw-first-heading\"><span class=\"mw-page-title-main\">Automatic summarization</span></h1>\\n\\t\\t\\t\\t\\t\\t\\t\\n<div id=\"p-lang-btn\" class=\"vector-dropdown mw-portlet mw-portlet-lang\"  >\\n\\t<input type=\"checkbox\" id=\"p-lang-btn-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-p-lang-btn\" class=\"vector-dropdown-checkbox mw-interlanguage-selector\" aria-label=\"Go to an article in another language. Available in 20 languages\"   >\\n\\t<label id=\"p-lang-btn-label\" for=\"p-lang-btn-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-20\" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive\"></span>\\n\\n<span class=\"vector-dropdown-label-text\">20 languages</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\t\\t<div class=\"vector-menu-content\">\\n\\t\\t\\t\\n\\t\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t<li class=\"interlanguage-link interwiki-ar mw-list-item\"><a href=\"https://ar.wikipedia.org/wiki/%D8%AA%D9%84%D8%AE%D9%8A%D8%B5_%D8%AA%D9%84%D9%82%D8%A7%D8%A6%D9%8A\" title=\"\\xd8\\xaa\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb5 \\xd8\\xaa\\xd9\\x84\\xd9\\x82\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a \\xe2\\x80\\x93 Arabic\" lang=\"ar\" hreflang=\"ar\" data-title=\"\\xd8\\xaa\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd8\\xb5 \\xd8\\xaa\\xd9\\x84\\xd9\\x82\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a\" data-language-autonym=\"\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9\" data-language-local-name=\"Arabic\" class=\"interlanguage-link-target\"><span>\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9</span></a></li><li class=\"interlanguage-link interwiki-bn mw-list-item\"><a href=\"https://bn.wikipedia.org/wiki/%E0%A6%B8%E0%A7%8D%E0%A6%AC%E0%A6%AF%E0%A6%BC%E0%A6%82%E0%A6%95%E0%A7%8D%E0%A6%B0%E0%A6%BF%E0%A6%AF%E0%A6%BC_%E0%A6%B8%E0%A6%82%E0%A6%95%E0%A7%8D%E0%A6%B7%E0%A6%BF%E0%A6%AA%E0%A7%8D%E0%A6%A4%E0%A6%95%E0%A6%B0%E0%A6%A3\" title=\"\\xe0\\xa6\\xb8\\xe0\\xa7\\x8d\\xe0\\xa6\\xac\\xe0\\xa6\\xaf\\xe0\\xa6\\xbc\\xe0\\xa6\\x82\\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\xb0\\xe0\\xa6\\xbf\\xe0\\xa6\\xaf\\xe0\\xa6\\xbc \\xe0\\xa6\\xb8\\xe0\\xa6\\x82\\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\xb7\\xe0\\xa6\\xbf\\xe0\\xa6\\xaa\\xe0\\xa7\\x8d\\xe0\\xa6\\xa4\\xe0\\xa6\\x95\\xe0\\xa6\\xb0\\xe0\\xa6\\xa3 \\xe2\\x80\\x93 Bangla\" lang=\"bn\" hreflang=\"bn\" data-title=\"\\xe0\\xa6\\xb8\\xe0\\xa7\\x8d\\xe0\\xa6\\xac\\xe0\\xa6\\xaf\\xe0\\xa6\\xbc\\xe0\\xa6\\x82\\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\xb0\\xe0\\xa6\\xbf\\xe0\\xa6\\xaf\\xe0\\xa6\\xbc \\xe0\\xa6\\xb8\\xe0\\xa6\\x82\\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\xb7\\xe0\\xa6\\xbf\\xe0\\xa6\\xaa\\xe0\\xa7\\x8d\\xe0\\xa6\\xa4\\xe0\\xa6\\x95\\xe0\\xa6\\xb0\\xe0\\xa6\\xa3\" data-language-autonym=\"\\xe0\\xa6\\xac\\xe0\\xa6\\xbe\\xe0\\xa6\\x82\\xe0\\xa6\\xb2\\xe0\\xa6\\xbe\" data-language-local-name=\"Bangla\" class=\"interlanguage-link-target\"><span>\\xe0\\xa6\\xac\\xe0\\xa6\\xbe\\xe0\\xa6\\x82\\xe0\\xa6\\xb2\\xe0\\xa6\\xbe</span></a></li><li class=\"interlanguage-link interwiki-de mw-list-item\"><a href=\"https://de.wikipedia.org/wiki/Text-Extraction\" title=\"Text-Extraction \\xe2\\x80\\x93 German\" lang=\"de\" hreflang=\"de\" data-title=\"Text-Extraction\" data-language-autonym=\"Deutsch\" data-language-local-name=\"German\" class=\"interlanguage-link-target\"><span>Deutsch</span></a></li><li class=\"interlanguage-link interwiki-eo mw-list-item\"><a href=\"https://eo.wikipedia.org/wiki/A%C5%ADtomata_noticado\" title=\"A\\xc5\\xadtomata noticado \\xe2\\x80\\x93 Esperanto\" lang=\"eo\" hreflang=\"eo\" data-title=\"A\\xc5\\xadtomata noticado\" data-language-autonym=\"Esperanto\" data-language-local-name=\"Esperanto\" class=\"interlanguage-link-target\"><span>Esperanto</span></a></li><li class=\"interlanguage-link interwiki-eu mw-list-item\"><a href=\"https://eu.wikipedia.org/wiki/Laburpengintza_automatikoa\" title=\"Laburpengintza automatikoa \\xe2\\x80\\x93 Basque\" lang=\"eu\" hreflang=\"eu\" data-title=\"Laburpengintza automatikoa\" data-language-autonym=\"Euskara\" data-language-local-name=\"Basque\" class=\"interlanguage-link-target\"><span>Euskara</span></a></li><li class=\"interlanguage-link interwiki-fa mw-list-item\"><a href=\"https://fa.wikipedia.org/wiki/%D8%AE%D9%84%D8%A7%D8%B5%D9%87%E2%80%8C%D8%B3%D8%A7%D8%B2%DB%8C_%D8%AE%D9%88%D8%AF%DA%A9%D8%A7%D8%B1\" title=\"\\xd8\\xae\\xd9\\x84\\xd8\\xa7\\xd8\\xb5\\xd9\\x87\\xe2\\x80\\x8c\\xd8\\xb3\\xd8\\xa7\\xd8\\xb2\\xdb\\x8c \\xd8\\xae\\xd9\\x88\\xd8\\xaf\\xda\\xa9\\xd8\\xa7\\xd8\\xb1 \\xe2\\x80\\x93 Persian\" lang=\"fa\" hreflang=\"fa\" data-title=\"\\xd8\\xae\\xd9\\x84\\xd8\\xa7\\xd8\\xb5\\xd9\\x87\\xe2\\x80\\x8c\\xd8\\xb3\\xd8\\xa7\\xd8\\xb2\\xdb\\x8c \\xd8\\xae\\xd9\\x88\\xd8\\xaf\\xda\\xa9\\xd8\\xa7\\xd8\\xb1\" data-language-autonym=\"\\xd9\\x81\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xdb\\x8c\" data-language-local-name=\"Persian\" class=\"interlanguage-link-target\"><span>\\xd9\\x81\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xdb\\x8c</span></a></li><li class=\"interlanguage-link interwiki-fr mw-list-item\"><a href=\"https://fr.wikipedia.org/wiki/R%C3%A9sum%C3%A9_automatique_de_texte\" title=\"R\\xc3\\xa9sum\\xc3\\xa9 automatique de texte \\xe2\\x80\\x93 French\" lang=\"fr\" hreflang=\"fr\" data-title=\"R\\xc3\\xa9sum\\xc3\\xa9 automatique de texte\" data-language-autonym=\"Fran\\xc3\\xa7ais\" data-language-local-name=\"French\" class=\"interlanguage-link-target\"><span>Fran\\xc3\\xa7ais</span></a></li><li class=\"interlanguage-link interwiki-ko mw-list-item\"><a href=\"https://ko.wikipedia.org/wiki/%EC%9E%90%EB%8F%99_%EC%9A%94%EC%95%BD\" title=\"\\xec\\x9e\\x90\\xeb\\x8f\\x99 \\xec\\x9a\\x94\\xec\\x95\\xbd \\xe2\\x80\\x93 Korean\" lang=\"ko\" hreflang=\"ko\" data-title=\"\\xec\\x9e\\x90\\xeb\\x8f\\x99 \\xec\\x9a\\x94\\xec\\x95\\xbd\" data-language-autonym=\"\\xed\\x95\\x9c\\xea\\xb5\\xad\\xec\\x96\\xb4\" data-language-local-name=\"Korean\" class=\"interlanguage-link-target\"><span>\\xed\\x95\\x9c\\xea\\xb5\\xad\\xec\\x96\\xb4</span></a></li><li class=\"interlanguage-link interwiki-ja mw-list-item\"><a href=\"https://ja.wikipedia.org/wiki/%E8%87%AA%E5%8B%95%E8%A6%81%E7%B4%84\" title=\"\\xe8\\x87\\xaa\\xe5\\x8b\\x95\\xe8\\xa6\\x81\\xe7\\xb4\\x84 \\xe2\\x80\\x93 Japanese\" lang=\"ja\" hreflang=\"ja\" data-title=\"\\xe8\\x87\\xaa\\xe5\\x8b\\x95\\xe8\\xa6\\x81\\xe7\\xb4\\x84\" data-language-autonym=\"\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e\" data-language-local-name=\"Japanese\" class=\"interlanguage-link-target\"><span>\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e</span></a></li><li class=\"interlanguage-link interwiki-nn mw-list-item\"><a href=\"https://nn.wikipedia.org/wiki/Automatisk_samandrag\" title=\"Automatisk samandrag \\xe2\\x80\\x93 Norwegian Nynorsk\" lang=\"nn\" hreflang=\"nn\" data-title=\"Automatisk samandrag\" data-language-autonym=\"Norsk nynorsk\" data-language-local-name=\"Norwegian Nynorsk\" class=\"interlanguage-link-target\"><span>Norsk nynorsk</span></a></li><li class=\"interlanguage-link interwiki-pt mw-list-item\"><a href=\"https://pt.wikipedia.org/wiki/Sumariza%C3%A7%C3%A3o_autom%C3%A1tica\" title=\"Sumariza\\xc3\\xa7\\xc3\\xa3o autom\\xc3\\xa1tica \\xe2\\x80\\x93 Portuguese\" lang=\"pt\" hreflang=\"pt\" data-title=\"Sumariza\\xc3\\xa7\\xc3\\xa3o autom\\xc3\\xa1tica\" data-language-autonym=\"Portugu\\xc3\\xaas\" data-language-local-name=\"Portuguese\" class=\"interlanguage-link-target\"><span>Portugu\\xc3\\xaas</span></a></li><li class=\"interlanguage-link interwiki-ro mw-list-item\"><a href=\"https://ro.wikipedia.org/wiki/Sumarizare_automat%C4%83\" title=\"Sumarizare automat\\xc4\\x83 \\xe2\\x80\\x93 Romanian\" lang=\"ro\" hreflang=\"ro\" data-title=\"Sumarizare automat\\xc4\\x83\" data-language-autonym=\"Rom\\xc3\\xa2n\\xc4\\x83\" data-language-local-name=\"Romanian\" class=\"interlanguage-link-target\"><span>Rom\\xc3\\xa2n\\xc4\\x83</span></a></li><li class=\"interlanguage-link interwiki-ru mw-list-item\"><a href=\"https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D1%80%D0%B5%D1%84%D0%B5%D1%80%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5\" title=\"\\xd0\\x90\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb5 \\xd1\\x80\\xd0\\xb5\\xd1\\x84\\xd0\\xb5\\xd1\\x80\\xd0\\xb8\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xe2\\x80\\x93 Russian\" lang=\"ru\" hreflang=\"ru\" data-title=\"\\xd0\\x90\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb5 \\xd1\\x80\\xd0\\xb5\\xd1\\x84\\xd0\\xb5\\xd1\\x80\\xd0\\xb8\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5\" data-language-autonym=\"\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9\" data-language-local-name=\"Russian\" class=\"interlanguage-link-target\"><span>\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9</span></a></li><li class=\"interlanguage-link interwiki-fi mw-list-item\"><a href=\"https://fi.wikipedia.org/wiki/Tekstin_automaattinen_tiivist%C3%A4minen\" title=\"Tekstin automaattinen tiivist\\xc3\\xa4minen \\xe2\\x80\\x93 Finnish\" lang=\"fi\" hreflang=\"fi\" data-title=\"Tekstin automaattinen tiivist\\xc3\\xa4minen\" data-language-autonym=\"Suomi\" data-language-local-name=\"Finnish\" class=\"interlanguage-link-target\"><span>Suomi</span></a></li><li class=\"interlanguage-link interwiki-sv mw-list-item\"><a href=\"https://sv.wikipedia.org/wiki/Textsammanfattning\" title=\"Textsammanfattning \\xe2\\x80\\x93 Swedish\" lang=\"sv\" hreflang=\"sv\" data-title=\"Textsammanfattning\" data-language-autonym=\"Svenska\" data-language-local-name=\"Swedish\" class=\"interlanguage-link-target\"><span>Svenska</span></a></li><li class=\"interlanguage-link interwiki-th mw-list-item\"><a href=\"https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%AD%E0%B8%B1%E0%B8%95%E0%B9%82%E0%B8%99%E0%B8%A1%E0%B8%B1%E0%B8%95%E0%B8%B4\" title=\"\\xe0\\xb8\\x81\\xe0\\xb8\\xb2\\xe0\\xb8\\xa3\\xe0\\xb8\\xaa\\xe0\\xb8\\xa3\\xe0\\xb8\\xb8\\xe0\\xb8\\x9b\\xe0\\xb8\\x84\\xe0\\xb8\\xa7\\xe0\\xb8\\xb2\\xe0\\xb8\\xa1\\xe0\\xb8\\xad\\xe0\\xb8\\xb1\\xe0\\xb8\\x95\\xe0\\xb9\\x82\\xe0\\xb8\\x99\\xe0\\xb8\\xa1\\xe0\\xb8\\xb1\\xe0\\xb8\\x95\\xe0\\xb8\\xb4 \\xe2\\x80\\x93 Thai\" lang=\"th\" hreflang=\"th\" data-title=\"\\xe0\\xb8\\x81\\xe0\\xb8\\xb2\\xe0\\xb8\\xa3\\xe0\\xb8\\xaa\\xe0\\xb8\\xa3\\xe0\\xb8\\xb8\\xe0\\xb8\\x9b\\xe0\\xb8\\x84\\xe0\\xb8\\xa7\\xe0\\xb8\\xb2\\xe0\\xb8\\xa1\\xe0\\xb8\\xad\\xe0\\xb8\\xb1\\xe0\\xb8\\x95\\xe0\\xb9\\x82\\xe0\\xb8\\x99\\xe0\\xb8\\xa1\\xe0\\xb8\\xb1\\xe0\\xb8\\x95\\xe0\\xb8\\xb4\" data-language-autonym=\"\\xe0\\xb9\\x84\\xe0\\xb8\\x97\\xe0\\xb8\\xa2\" data-language-local-name=\"Thai\" class=\"interlanguage-link-target\"><span>\\xe0\\xb9\\x84\\xe0\\xb8\\x97\\xe0\\xb8\\xa2</span></a></li><li class=\"interlanguage-link interwiki-tr mw-list-item\"><a href=\"https://tr.wikipedia.org/wiki/Otomatik_%C3%B6zetleme\" title=\"Otomatik \\xc3\\xb6zetleme \\xe2\\x80\\x93 Turkish\" lang=\"tr\" hreflang=\"tr\" data-title=\"Otomatik \\xc3\\xb6zetleme\" data-language-autonym=\"T\\xc3\\xbcrk\\xc3\\xa7e\" data-language-local-name=\"Turkish\" class=\"interlanguage-link-target\"><span>T\\xc3\\xbcrk\\xc3\\xa7e</span></a></li><li class=\"interlanguage-link interwiki-uk mw-list-item\"><a href=\"https://uk.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B5_%D1%80%D0%B5%D1%84%D0%B5%D1%80%D1%83%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F\" title=\"\\xd0\\x90\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd1\\x82\\xd0\\xb8\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb5 \\xd1\\x80\\xd0\\xb5\\xd1\\x84\\xd0\\xb5\\xd1\\x80\\xd1\\x83\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xe2\\x80\\x93 Ukrainian\" lang=\"uk\" hreflang=\"uk\" data-title=\"\\xd0\\x90\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd1\\x82\\xd0\\xb8\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb5 \\xd1\\x80\\xd0\\xb5\\xd1\\x84\\xd0\\xb5\\xd1\\x80\\xd1\\x83\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f\" data-language-autonym=\"\\xd0\\xa3\\xd0\\xba\\xd1\\x80\\xd0\\xb0\\xd1\\x97\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb0\" data-language-local-name=\"Ukrainian\" class=\"interlanguage-link-target\"><span>\\xd0\\xa3\\xd0\\xba\\xd1\\x80\\xd0\\xb0\\xd1\\x97\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb0</span></a></li><li class=\"interlanguage-link interwiki-vi mw-list-item\"><a href=\"https://vi.wikipedia.org/wiki/T%C3%B3m_t%E1%BA%AFt_t%E1%BB%B1_%C4%91%E1%BB%99ng\" title=\"T\\xc3\\xb3m t\\xe1\\xba\\xaft t\\xe1\\xbb\\xb1 \\xc4\\x91\\xe1\\xbb\\x99ng \\xe2\\x80\\x93 Vietnamese\" lang=\"vi\" hreflang=\"vi\" data-title=\"T\\xc3\\xb3m t\\xe1\\xba\\xaft t\\xe1\\xbb\\xb1 \\xc4\\x91\\xe1\\xbb\\x99ng\" data-language-autonym=\"Ti\\xe1\\xba\\xbfng Vi\\xe1\\xbb\\x87t\" data-language-local-name=\"Vietnamese\" class=\"interlanguage-link-target\"><span>Ti\\xe1\\xba\\xbfng Vi\\xe1\\xbb\\x87t</span></a></li><li class=\"interlanguage-link interwiki-zh-yue mw-list-item\"><a href=\"https://zh-yue.wikipedia.org/wiki/%E8%87%AA%E5%8B%95%E7%B8%BD%E7%B5%90\" title=\"\\xe8\\x87\\xaa\\xe5\\x8b\\x95\\xe7\\xb8\\xbd\\xe7\\xb5\\x90 \\xe2\\x80\\x93 Cantonese\" lang=\"yue\" hreflang=\"yue\" data-title=\"\\xe8\\x87\\xaa\\xe5\\x8b\\x95\\xe7\\xb8\\xbd\\xe7\\xb5\\x90\" data-language-autonym=\"\\xe7\\xb2\\xb5\\xe8\\xaa\\x9e\" data-language-local-name=\"Cantonese\" class=\"interlanguage-link-target\"><span>\\xe7\\xb2\\xb5\\xe8\\xaa\\x9e</span></a></li>\\n\\t\\t\\t</ul>\\n\\t\\t\\t<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-edit wb-langlinks-link\"><a href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q1394144#sitelinks-wikipedia\" title=\"Edit interlanguage links\" class=\"wbc-editpage\">Edit links</a></span></div>\\n\\t\\t</div>\\n\\n\\t</div>\\n</div>\\n</header>\\n\\t\\t\\t\\t<div class=\"vector-page-toolbar\">\\n\\t\\t\\t\\t\\t<div class=\"vector-page-toolbar-container\">\\n\\t\\t\\t\\t\\t\\t<div id=\"left-navigation\">\\n\\t\\t\\t\\t\\t\\t\\t<nav aria-label=\"Namespaces\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n<div id=\"p-associated-pages\" class=\"vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"ca-nstab-main\" class=\"selected vector-tab-noicon mw-list-item\"><a href=\"/wiki/Automatic_summarization\" title=\"View the content page [c]\" accesskey=\"c\"><span>Article</span></a></li><li id=\"ca-talk\" class=\"vector-tab-noicon mw-list-item\"><a href=\"/wiki/Talk:Automatic_summarization\" rel=\"discussion\" title=\"Discuss improvements to the content page [t]\" accesskey=\"t\"><span>Talk</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n<div id=\"vector-variants-dropdown\" class=\"vector-dropdown emptyPortlet\"  >\\n\\t<input type=\"checkbox\" id=\"vector-variants-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-variants-dropdown\" class=\"vector-dropdown-checkbox \" aria-label=\"Change language variant\"   >\\n\\t<label id=\"vector-variants-dropdown-label\" for=\"vector-variants-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet\" aria-hidden=\"true\"  ><span class=\"vector-dropdown-label-text\">English</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\n\\t\\t\\t\\t\\t\\n<div id=\"p-variants\" class=\"vector-menu mw-portlet mw-portlet-variants emptyPortlet\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t</nav>\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<div id=\"right-navigation\" class=\"vector-collapsible\">\\n\\t\\t\\t\\t\\t\\t\\t<nav aria-label=\"Views\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n<div id=\"p-views\" class=\"vector-menu vector-menu-tabs mw-portlet mw-portlet-views\"  >\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"ca-view\" class=\"selected vector-tab-noicon mw-list-item\"><a href=\"/wiki/Automatic_summarization\"><span>Read</span></a></li><li id=\"ca-edit\" class=\"vector-tab-noicon mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit\" title=\"Edit this page [e]\" accesskey=\"e\"><span>Edit</span></a></li><li id=\"ca-history\" class=\"vector-tab-noicon mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;action=history\" title=\"Past revisions of this page [h]\" accesskey=\"h\"><span>View history</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t</nav>\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t<nav class=\"vector-page-tools-landmark\" aria-label=\"Page tools\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n<div id=\"vector-page-tools-dropdown\" class=\"vector-dropdown vector-page-tools-dropdown\"  >\\n\\t<input type=\"checkbox\" id=\"vector-page-tools-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-page-tools-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Tools\"  >\\n\\t<label id=\"vector-page-tools-dropdown-label\" for=\"vector-page-tools-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet\" aria-hidden=\"true\"  ><span class=\"vector-dropdown-label-text\">Tools</span>\\n\\t</label>\\n\\t<div class=\"vector-dropdown-content\">\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"vector-page-tools-unpinned-container\" class=\"vector-unpinned-container\">\\n\\t\\t\\t\\t\\t\\t\\n<div id=\"vector-page-tools\" class=\"vector-page-tools vector-pinnable-element\">\\n\\t<div\\n\\tclass=\"vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned\"\\n\\tdata-feature-name=\"page-tools-pinned\"\\n\\tdata-pinnable-element-id=\"vector-page-tools\"\\n\\tdata-pinned-container-id=\"vector-page-tools-pinned-container\"\\n\\tdata-unpinned-container-id=\"vector-page-tools-unpinned-container\"\\n>\\n\\t<div class=\"vector-pinnable-header-label\">Tools</div>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-page-tools.pin\">move to sidebar</button>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-page-tools.unpin\">hide</button>\\n</div>\\n\\n\\t\\n<div id=\"p-cactions\" class=\"vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items\"  title=\"More options\" >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tActions\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"ca-more-view\" class=\"selected vector-more-collapsible-item mw-list-item\"><a href=\"/wiki/Automatic_summarization\"><span>Read</span></a></li><li id=\"ca-more-edit\" class=\"vector-more-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit\" title=\"Edit this page [e]\" accesskey=\"e\"><span>Edit</span></a></li><li id=\"ca-more-history\" class=\"vector-more-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;action=history\"><span>View history</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n<div id=\"p-tb\" class=\"vector-menu mw-portlet mw-portlet-tb\"  >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tGeneral\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"t-whatlinkshere\" class=\"mw-list-item\"><a href=\"/wiki/Special:WhatLinksHere/Automatic_summarization\" title=\"List of all English Wikipedia pages containing links to this page [j]\" accesskey=\"j\"><span>What links here</span></a></li><li id=\"t-recentchangeslinked\" class=\"mw-list-item\"><a href=\"/wiki/Special:RecentChangesLinked/Automatic_summarization\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\" accesskey=\"k\"><span>Related changes</span></a></li><li id=\"t-upload\" class=\"mw-list-item\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\" accesskey=\"u\"><span>Upload file</span></a></li><li id=\"t-permalink\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;oldid=1236297853\" title=\"Permanent link to this revision of this page\"><span>Permanent link</span></a></li><li id=\"t-info\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;action=info\" title=\"More information about this page\"><span>Page information</span></a></li><li id=\"t-cite\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Automatic_summarization&amp;id=1236297853&amp;wpFormIdentifier=titleform\" title=\"Information on how to cite this page\"><span>Cite this page</span></a></li><li id=\"t-urlshortener\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAutomatic_summarization\"><span>Get shortened URL</span></a></li><li id=\"t-urlshortener-qrcode\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAutomatic_summarization\"><span>Download QR code</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n<div id=\"p-coll-print_export\" class=\"vector-menu mw-portlet mw-portlet-coll-print_export\"  >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tPrint/export\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"coll-download-as-rl\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:DownloadAsPdf&amp;page=Automatic_summarization&amp;action=show-download-screen\" title=\"Download this page as a PDF file\"><span>Download as PDF</span></a></li><li id=\"t-print\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Automatic_summarization&amp;printable=yes\" title=\"Printable version of this page [p]\" accesskey=\"p\"><span>Printable version</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n<div id=\"p-wikibase-otherprojects\" class=\"vector-menu mw-portlet mw-portlet-wikibase-otherprojects\"  >\\n\\t<div class=\"vector-menu-heading\">\\n\\t\\tIn other projects\\n\\t</div>\\n\\t<div class=\"vector-menu-content\">\\n\\t\\t\\n\\t\\t<ul class=\"vector-menu-content-list\">\\n\\t\\t\\t\\n\\t\\t\\t<li id=\"t-wikibase\" class=\"wb-otherproject-link wb-otherproject-wikibase-dataitem mw-list-item\"><a href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q1394144\" title=\"Structured data on this page hosted by Wikidata [g]\" accesskey=\"g\"><span>Wikidata item</span></a></li>\\n\\t\\t</ul>\\n\\t\\t\\n\\t</div>\\n</div>\\n\\n</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\n\\t</div>\\n</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t</nav>\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t<div class=\"vector-column-end\">\\n\\t\\t\\t\\t\\t<div class=\"vector-sticky-pinned-container\">\\n\\t\\t\\t\\t\\t\\t<nav class=\"vector-page-tools-landmark\" aria-label=\"Page tools\">\\n\\t\\t\\t\\t\\t\\t\\t<div id=\"vector-page-tools-pinned-container\" class=\"vector-pinned-container\">\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</nav>\\n\\t\\t\\t\\t\\t\\t<nav class=\"vector-appearance-landmark\" aria-label=\"Appearance\">\\n\\t\\t\\t\\t\\t\\t\\t<div id=\"vector-appearance-pinned-container\" class=\"vector-pinned-container\">\\n\\t\\t\\t\\t<div id=\"vector-appearance\" class=\"vector-appearance vector-pinnable-element\">\\n\\t<div\\n\\tclass=\"vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned\"\\n\\tdata-feature-name=\"appearance-pinned\"\\n\\tdata-pinnable-element-id=\"vector-appearance\"\\n\\tdata-pinned-container-id=\"vector-appearance-pinned-container\"\\n\\tdata-unpinned-container-id=\"vector-appearance-unpinned-container\"\\n>\\n\\t<div class=\"vector-pinnable-header-label\">Appearance</div>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-appearance.pin\">move to sidebar</button>\\n\\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-appearance.unpin\">hide</button>\\n</div>\\n\\n\\n</div>\\n\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</nav>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t<div id=\"bodyContent\" class=\"vector-body\" aria-labelledby=\"firstHeading\" data-mw-ve-target-container>\\n\\t\\t\\t\\t\\t<div class=\"vector-body-before-content\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"mw-indicators\">\\n\\t\\t</div>\\n\\n\\t\\t\\t\\t\\t\\t<div id=\"siteSub\" class=\"noprint\">From Wikipedia, the free encyclopedia</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t<div id=\"contentSub\"><div id=\"mw-content-subtitle\"></div></div>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<div id=\"mw-content-text\" class=\"mw-body-content\"><div class=\"mw-content-ltr mw-parser-output\" lang=\"en\" dir=\"ltr\"><div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Computer-based method for summarizing a text</div>\\n<style data-mw-deduplicate=\"TemplateStyles:r1251242444\">.mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}@media print{body.ns-0 .mw-parser-output .ambox{display:none!important}}</style><table class=\"box-More_citations_needed plainlinks metadata ambox ambox-content ambox-Refimprove\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div class=\"mbox-image-div\"><span typeof=\"mw:File\"><a href=\"/wiki/File:Question_book-new.svg\" class=\"mw-file-description\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" decoding=\"async\" width=\"50\" height=\"39\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x\" data-file-width=\"512\" data-file-height=\"399\" /></a></span></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>needs additional citations for <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">verification</a></b>.<span class=\"hide-when-compact\"> Please help <a href=\"/wiki/Special:EditPage/Automatic_summarization\" title=\"Special:EditPage/Automatic summarization\">improve this article</a> by <a href=\"/wiki/Help:Referencing_for_beginners\" title=\"Help:Referencing for beginners\">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.<br /><small><span class=\"plainlinks\"><i>Find sources:</i>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://www.google.com/search?as_eq=wikipedia&amp;q=%22Automatic+summarization%22\">\"Automatic summarization\"</a>&#160;\\xe2\\x80\\x93&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://www.google.com/search?tbm=nws&amp;q=%22Automatic+summarization%22+-wikipedia&amp;tbs=ar:1\">news</a>&#160;<b>\\xc2\\xb7</b> <a rel=\"nofollow\" class=\"external text\" href=\"https://www.google.com/search?&amp;q=%22Automatic+summarization%22&amp;tbs=bkt:s&amp;tbm=bks\">newspapers</a>&#160;<b>\\xc2\\xb7</b> <a rel=\"nofollow\" class=\"external text\" href=\"https://www.google.com/search?tbs=bks:1&amp;q=%22Automatic+summarization%22+-wikipedia\">books</a>&#160;<b>\\xc2\\xb7</b> <a rel=\"nofollow\" class=\"external text\" href=\"https://scholar.google.com/scholar?q=%22Automatic+summarization%22\">scholar</a>&#160;<b>\\xc2\\xb7</b> <a rel=\"nofollow\" class=\"external text\" href=\"https://www.jstor.org/action/doBasicSearch?Query=%22Automatic+summarization%22&amp;acc=on&amp;wc=on\">JSTOR</a></span></small></span>  <span class=\"date-container\"><i>(<span class=\"date\">April 2022</span>)</i></span><span class=\"hide-when-compact\"><i> (<small><a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this message</a></small>)</i></span></div></td></tr></tbody></table>\\n<p><b>Automatic summarization</b> is the process of shortening a set of data computationally, to create a subset (a <a href=\"/wiki/Abstract_(summary)\" title=\"Abstract (summary)\">summary</a>) that represents the most important or relevant information within the original content. <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">Artificial intelligence</a> <a href=\"/wiki/Algorithm\" title=\"Algorithm\">algorithms</a> are commonly developed and employed to achieve this, specialized for different types of data.\\n</p><p><a href=\"/wiki/Plain_text\" title=\"Plain text\">Text</a> summarization is usually implemented by <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a> methods, designed to locate the most informative sentences in a given document.<sup id=\"cite_ref-Torres2014_1-0\" class=\"reference\"><a href=\"#cite_note-Torres2014-1\"><span class=\"cite-bracket\">&#91;</span>1<span class=\"cite-bracket\">&#93;</span></a></sup> On the other hand, visual content can be summarized using <a href=\"/wiki/Computer_vision\" title=\"Computer vision\">computer vision</a> algorithms. <a href=\"/wiki/Image\" title=\"Image\">Image</a> summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\"><span class=\"cite-bracket\">&#91;</span>2<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\"><span class=\"cite-bracket\">&#91;</span>3<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\"><span class=\"cite-bracket\">&#91;</span>4<span class=\"cite-bracket\">&#93;</span></a></sup> Video summarization algorithms identify and extract from the original video content the most important frames (<i>key-frames</i>), and/or the most important video segments (<i>key-shots</i>), normally in a temporally ordered fashion.<sup id=\"cite_ref-PalPetrosino2012_5-0\" class=\"reference\"><a href=\"#cite_note-PalPetrosino2012-5\"><span class=\"cite-bracket\">&#91;</span>5<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-Elhamifar2012_6-0\" class=\"reference\"><a href=\"#cite_note-Elhamifar2012-6\"><span class=\"cite-bracket\">&#91;</span>6<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-Mademlis2016_7-0\" class=\"reference\"><a href=\"#cite_note-Mademlis2016-7\"><span class=\"cite-bracket\">&#91;</span>7<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-Mademlis2018_8-0\" class=\"reference\"><a href=\"#cite_note-Mademlis2018-8\"><span class=\"cite-bracket\">&#91;</span>8<span class=\"cite-bracket\">&#93;</span></a></sup> Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of <a href=\"/wiki/Video_synopsis\" title=\"Video synopsis\">video synopsis</a> algorithms, where <i>new</i> video frames are being synthesized based on the original video content.\\n</p>\\n<meta property=\"mw:PageProp/toc\" />\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"Commercial_products\">Commercial products</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=1\" title=\"Edit section: Commercial products\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>In 2022 <a href=\"/wiki/Google_Docs\" title=\"Google Docs\">Google Docs</a> released an automatic summarization feature.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\"><span class=\"cite-bracket\">&#91;</span>9<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"Approaches\">Approaches</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=2\" title=\"Edit section: Approaches\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>There are two general approaches to automatic summarization: <a href=\"/wiki/Information_extraction\" title=\"Information extraction\">extraction</a> and <a href=\"/wiki/Abstract_(summary)\" title=\"Abstract (summary)\">abstraction</a>.\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Extraction-based_summarization\">Extraction-based summarization</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=3\" title=\"Edit section: Extraction-based summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\"><span class=\"cite-bracket\">&#91;</span>10<span class=\"cite-bracket\">&#93;</span></a></sup> Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).<sup id=\"cite_ref-Afzal_et_al_11-0\" class=\"reference\"><a href=\"#cite_note-Afzal_et_al-11\"><span class=\"cite-bracket\">&#91;</span>11<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Abstractive-based_summarization\">Abstractive-based summarization</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=4\" title=\"Edit section: Abstractive-based summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Abstractive summarization methods generate new text that did not exist in the original text.<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\"><span class=\"cite-bracket\">&#91;</span>12<span class=\"cite-bracket\">&#93;</span></a></sup> This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by <a href=\"/wiki/Automated_paraphrasing\" class=\"mw-redirect\" title=\"Automated paraphrasing\">paraphrasing</a> sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a> and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to images and videos, which is why most summarization systems are extractive.\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Aided_summarization\">Aided summarization</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=5\" title=\"Edit section: Aided summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\\n</p>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"Applications_and_systems_for_summarization\">Applications and systems for summarization</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=6\" title=\"Edit section: Applications and systems for summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is <i>generic summarization</i>, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  <i>query relevant summarization</i>, sometimes called <i>query-based summarization</i>, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\n</p><p>An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a <a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">cluster</a> of articles on the same topic). This problem is called <a href=\"/wiki/Multi-document_summarization\" title=\"Multi-document summarization\">multi-document summarization</a>. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\\n</p><p>Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.<sup id=\"cite_ref-13\" class=\"reference\"><a href=\"#cite_note-13\"><span class=\"cite-bracket\">&#91;</span>13<span class=\"cite-bracket\">&#93;</span></a></sup> A summary in this context is useful to show the most representative images of results in an <a href=\"/wiki/Image_collection_exploration\" title=\"Image collection exploration\">image collection exploration</a> system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\\n</p><p>At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the <i>core-set</i>. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, <a href=\"/wiki/Submodular_set_function\" title=\"Submodular set function\">Submodular set function</a>, <a href=\"/wiki/Determinantal_point_process\" title=\"Determinantal point process\">Determinantal point process</a>, maximal marginal relevance (MMR) etc.\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Keyphrase_extraction\">Keyphrase extraction</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=7\" title=\"Edit section: Keyphrase extraction\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>The task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.<sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\"><span class=\"cite-bracket\">&#91;</span>14<span class=\"cite-bracket\">&#93;</span></a></sup> In the case of <a href=\"/wiki/Research_article\" class=\"mw-redirect\" title=\"Research article\">research articles</a>, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\\nConsider the example text from a news article:\\n</p>\\n<dl><dd>\"The Army Corps of Engineers, rushing to meet President Bush\\'s promise to protect New Orleans by the start of the 2006 hurricane season, installed defective flood-control pumps last year despite warnings from its own expert that the equipment would fail during a storm, according to documents obtained by The Associated Press\".</dd></dl>\\n<p>A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep <a href=\"/wiki/Natural-language_understanding\" class=\"mw-redirect\" title=\"Natural-language understanding\">understanding of the text</a>, which makes it difficult for a computer system.\\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a> (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a <a href=\"/wiki/Full-text_search\" title=\"Full-text search\">full-text search</a>), and be employed in generating index entries for a large text corpus.\\n</p><p>Depending on the different literature and the definition of key terms, words or phrases, <a href=\"/wiki/Keyword_extraction\" title=\"Keyword extraction\">keyword extraction</a> is a highly related theme.\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"Supervised_learning_approaches\">Supervised learning approaches</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=8\" title=\"Edit section: Supervised learning approaches\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Beginning with the work of Turney,<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\"><span class=\"cite-bracket\">&#91;</span>15<span class=\"cite-bracket\">&#93;</span></a></sup> many researchers have approached keyphrase extraction as a <a href=\"/wiki/Supervised_machine_learning\" class=\"mw-redirect\" title=\"Supervised machine learning\">supervised machine learning</a> problem.\\nGiven a document, we construct an example for each <a href=\"/wiki/Unigram\" class=\"mw-redirect\" title=\"Unigram\">unigram</a>, <a href=\"/wiki/Bigram\" title=\"Bigram\">bigram</a>, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a <a href=\"/wiki/Binary_classification\" title=\"Binary classification\">binary classification</a> for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\\nKeyphrase extractors are generally evaluated using <a href=\"/wiki/Precision_and_recall\" title=\"Precision and recall\">precision and recall</a>. Precision measures how\\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\\nharmonic mean of the two (<i>F</i>&#160;=&#160;2<i>PR</i>/(<i>P</i>&#160;+&#160;<i>R</i>) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\\n</p><p>Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\\n</p><p>We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney\\'s seminal paper.\\n</p><p>In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\\n</p><p>Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, <a href=\"/wiki/Naive_Bayes\" class=\"mw-redirect\" title=\"Naive Bayes\">Naive Bayes</a>, and rule induction. In the case of Turney\\'s GenEx algorithm, a <a href=\"/wiki/Genetic_algorithm\" title=\"Genetic algorithm\">genetic algorithm</a> is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"Unsupervised_approach:_TextRank\">Unsupervised approach: TextRank</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=9\" title=\"Edit section: Unsupervised approach: TextRank\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of <a href=\"/wiki/Training_set\" class=\"mw-redirect\" title=\"Training set\">training data</a>. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney\\'s results demonstrate.\\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\"><span class=\"cite-bracket\">&#91;</span>16<span class=\"cite-bracket\">&#93;</span></a></sup> exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that <a href=\"/wiki/PageRank\" title=\"PageRank\">PageRank</a> selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from <a href=\"/wiki/Social_network\" title=\"Social network\">social networks</a>. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text\\'s intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\\n</p><p>TextRank is a general purpose <a href=\"/wiki/Graph_(abstract_data_type)\" title=\"Graph (abstract data type)\">graph</a>-based ranking algorithm for <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">NLP</a>. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or <a href=\"/wiki/Lexical_(semiotics)\" class=\"mw-redirect\" title=\"Lexical (semiotics)\">lexical</a> <a href=\"/wiki/Semantic_similarity\" title=\"Semantic similarity\">similarity</a> between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to <a href=\"/wiki/Eigenvalue\" class=\"mw-redirect\" title=\"Eigenvalue\">eigenvalue</a> 1 (i.e., the <a href=\"/wiki/Stationary_distribution\" title=\"Stationary distribution\">stationary distribution</a> of the <a href=\"/wiki/Random_walk\" title=\"Random walk\">random walk</a> on the graph).\\n</p><p>The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\\n</p><p>Edges are created based on word <a href=\"/wiki/Co-occurrence\" title=\"Co-occurrence\">co-occurrence</a> in this application of TextRank. Two vertices are connected by an edge if the <a href=\"/wiki/Unigram\" class=\"mw-redirect\" title=\"Unigram\">unigrams</a> appear within a window of size N in the original text. N is typically around 2\\xe2\\x80\\x9310. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text <a href=\"/wiki/Cohesion_(linguistics)\" title=\"Cohesion (linguistics)\">cohesion</a>\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\\n</p><p>Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\\n</p><p>It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\\n</p><p>In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Document_summarization\">Document summarization</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=10\" title=\"Edit section: Document summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Like keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units\\xe2\\x80\\x94whole sentences instead of words and phrases.\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"Supervised_learning_approaches_2\">Supervised learning approaches</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=11\" title=\"Edit section: Supervised learning approaches\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Supervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 evaluation only considers unigrams.\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"Maximum_entropy-based_summarization\">Maximum entropy-based summarization</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=12\" title=\"Edit section: Maximum entropy-based summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>During the DUC 2001 and 2002 evaluation workshops, <a href=\"/wiki/Netherlands_Organisation_for_Applied_Scientific_Research\" title=\"Netherlands Organisation for Applied Scientific Research\">TNO</a> developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a <a href=\"/wiki/Naive_Bayes_classifier\" title=\"Naive Bayes classifier\">Naive Bayes classifier</a> and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a <a href=\"/wiki/Maximum_entropy_classifier\" class=\"mw-redirect\" title=\"Maximum entropy classifier\">maximum entropy</a> (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"Adaptive_summarization\">Adaptive summarization</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=13\" title=\"Edit section: Adaptive summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>A promising approach is adaptive document/text summarization.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\"><span class=\"cite-bracket\">&#91;</span>17<span class=\"cite-bracket\">&#93;</span></a></sup> It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre. Such software has been created.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\"><span class=\"cite-bracket\">&#91;</span>18<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"TextRank_and_LexRank\">TextRank and LexRank</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=14\" title=\"Edit section: TextRank and LexRank\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"<a href=\"/wiki/Centroid\" title=\"Centroid\">centroid</a>\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\\n</p><p>A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank<sup id=\"cite_ref-19\" class=\"reference\"><a href=\"#cite_note-19\"><span class=\"cite-bracket\">&#91;</span>19<span class=\"cite-bracket\">&#93;</span></a></sup> is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\\n</p><p>In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\\n</p><p>The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses <a href=\"/wiki/Cosine_similarity\" title=\"Cosine similarity\">cosine similarity</a> of <a href=\"/wiki/TF-IDF\" class=\"mw-redirect\" title=\"TF-IDF\">TF-IDF</a> vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (<a href=\"/wiki/Quantile_normalization\" title=\"Quantile normalization\">normalized</a> by the sentences\\' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous <a href=\"/wiki/Similarity_score\" title=\"Similarity score\">similarity scores</a> as weights.\\n</p><p>In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\\n</p><p>It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (<a href=\"/w/index.php?title=MEAD&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"MEAD (page does not exist)\">MEAD</a>) that combines the LexRank score (stationary probability) with other features like sentence position and length using a <a href=\"/wiki/Linear_combination\" title=\"Linear combination\">linear combination</a> with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\\n</p><p>Unlike TextRank, LexRank has been applied to multi-document summarization.\\n</p>\\n<div class=\"mw-heading mw-heading4\"><h4 id=\"Multi-document_summarization\">Multi-document summarization</h4><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=15\" title=\"Edit section: Multi-document summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<style data-mw-deduplicate=\"TemplateStyles:r1236090951\">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}</style><div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Multi-document_summarization\" title=\"Multi-document summarization\">Multi-document summarization</a></div>\\n<p><b>Multi-document summarization</b> is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the <a href=\"/wiki/News_aggregators\" class=\"mw-redirect\" title=\"News aggregators\">news aggregators</a> performing the next step down the road of coping with <a href=\"/wiki/Information_overload\" title=\"Information overload\">information overload</a>. Multi-document summarization may also be done in response to a question.<sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\"><span class=\"cite-bracket\">&#91;</span>20<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-Afzal_et_al_11-1\" class=\"reference\"><a href=\"#cite_note-Afzal_et_al-11\"><span class=\"cite-bracket\">&#91;</span>11<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p><p>Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. <sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Accuracy_dispute#Disputed_statement\" title=\"Wikipedia:Accuracy dispute\"><span title=\"The material near this tag is possibly inaccurate or nonfactual. (June 2018)\">dubious</span></a>&#32;&#8211; <a href=\"/wiki/Talk:Automatic_summarization#Dubious\" title=\"Talk:Automatic summarization\">discuss</a></i>&#93;</sup>\\n</p>\\n<div class=\"mw-heading mw-heading5\"><h5 id=\"Diversity\">Diversity</h5><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=16\" title=\"Edit section: Diversity\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Multi-document extractive summarization faces a problem of redundancy. Ideally, we want to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). For example, in a set of news articles about some event, each article is likely to have many similar sentences. To address this issue, LexRank applies a heuristic post-processing step that adds sentences in rank order, but discards sentences that are too similar to ones already in the summary. This method is called Cross-Sentence Information Subsumption (CSIS). These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. Its importance also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to an arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\\n</p><p>A related method is Maximal Marginal Relevance (MMR),<sup id=\"cite_ref-21\" class=\"reference\"><a href=\"#cite_note-21\"><span class=\"cite-bracket\">&#91;</span>21<span class=\"cite-bracket\">&#93;</span></a></sup> which uses a general-purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on <a href=\"/wiki/Absorbing_Markov_chain\" title=\"Absorbing Markov chain\">absorbing Markov chain</a> random walks (a random walk where certain states end the walk). The algorithm is called GRASSHOPPER.<sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\"><span class=\"cite-bracket\">&#91;</span>22<span class=\"cite-bracket\">&#93;</span></a></sup> In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\\n</p><p>The state of the art results for multi-document summarization are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.<sup id=\"cite_ref-23\" class=\"reference\"><a href=\"#cite_note-23\"><span class=\"cite-bracket\">&#91;</span>23<span class=\"cite-bracket\">&#93;</span></a></sup> Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\"><span class=\"cite-bracket\">&#91;</span>24<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p><p>A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position. It does not use word frequency, training or preprocessing. It uses two user-supplied parameters: equivalence (when are two sentences to be considered equivalent?) and relevance (how long is the desired summary?).\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Submodular_functions_as_generic_tools_for_summarization\">Submodular functions as generic tools for summarization</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=17\" title=\"Edit section: Submodular functions as generic tools for summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>The idea of a <a href=\"/wiki/Submodular_set_function\" title=\"Submodular set function\">submodular set function</a> has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of <i>coverage</i>, <i>information</i>, <i>representation</i> and <i>diversity</i>. Moreover, several important <a href=\"/wiki/Combinatorial_optimization\" title=\"Combinatorial optimization\">combinatorial optimization</a> problems occur as special instances of submodular optimization. For example, the <a href=\"/wiki/Set_cover_problem\" title=\"Set cover problem\">set cover problem</a> is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which <i>cover</i> a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the <a href=\"/wiki/Optimal_facility_location\" title=\"Optimal facility location\">facility location problem</a> is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a <a href=\"/wiki/Determinantal_point_process\" title=\"Determinantal point process\">determinantal point process</a> to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\\n</p><p>While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple <a href=\"/wiki/Greedy_algorithm\" title=\"Greedy algorithm\">greedy algorithm</a> admits a constant factor guarantee.<sup id=\"cite_ref-25\" class=\"reference\"><a href=\"#cite_note-25\"><span class=\"cite-bracket\">&#91;</span>25<span class=\"cite-bracket\">&#93;</span></a></sup> Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\\n</p><p>Submodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\"><span class=\"cite-bracket\">&#91;</span>26<span class=\"cite-bracket\">&#93;</span></a></sup> shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,<sup id=\"cite_ref-27\" class=\"reference\"><a href=\"#cite_note-27\"><span class=\"cite-bracket\">&#91;</span>27<span class=\"cite-bracket\">&#93;</span></a></sup> shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (June 2018)\">citation needed</span></a></i>&#93;</sup>\\n</p><p>Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show<sup id=\"cite_ref-28\" class=\"reference\"><a href=\"#cite_note-28\"><span class=\"cite-bracket\">&#91;</span>28<span class=\"cite-bracket\">&#93;</span></a></sup> that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015<sup id=\"cite_ref-29\" class=\"reference\"><a href=\"#cite_note-29\"><span class=\"cite-bracket\">&#91;</span>29<span class=\"cite-bracket\">&#93;</span></a></sup> show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.<sup id=\"cite_ref-30\" class=\"reference\"><a href=\"#cite_note-30\"><span class=\"cite-bracket\">&#91;</span>30<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Applications\">Applications</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=18\" title=\"Edit section: Applications\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1251242444\" /><table class=\"box-Expand_section plainlinks metadata ambox mbox-small-left ambox-content\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><span typeof=\"mw:File\"><a href=\"/wiki/File:Wiki_letter_w_cropped.svg\" class=\"mw-file-description\"><img alt=\"[icon]\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/20px-Wiki_letter_w_cropped.svg.png\" decoding=\"async\" width=\"20\" height=\"14\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/30px-Wiki_letter_w_cropped.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/40px-Wiki_letter_w_cropped.svg.png 2x\" data-file-width=\"44\" data-file-height=\"31\" /></a></span></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This section <b>needs expansion</b>. You can help by <a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=\">adding to it</a>.  <span class=\"date-container\"><i>(<span class=\"date\">February 2017</span>)</i></span></div></td></tr></tbody></table>\\n<p>Specific applications of automatic summarization include:\\n</p>\\n<ul><li>The <a href=\"/wiki/Reddit\" title=\"Reddit\">Reddit</a> <a href=\"/wiki/Internet_bot\" title=\"Internet bot\">bot</a> \"autotldr\",<sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\"><span class=\"cite-bracket\">&#91;</span>31<span class=\"cite-bracket\">&#93;</span></a></sup> created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\"><span class=\"cite-bracket\">&#91;</span>32<span class=\"cite-bracket\">&#93;</span></a></sup> The name is reference to <a href=\"/wiki/TL;DR\" title=\"TL;DR\">TL;DR</a> \\xe2\\x88\\x92 <a href=\"/wiki/Internet_slang\" title=\"Internet slang\">Internet slang</a> for \"too long; didn\\'t read\".<sup id=\"cite_ref-33\" class=\"reference\"><a href=\"#cite_note-33\"><span class=\"cite-bracket\">&#91;</span>33<span class=\"cite-bracket\">&#93;</span></a></sup><sup id=\"cite_ref-34\" class=\"reference\"><a href=\"#cite_note-34\"><span class=\"cite-bracket\">&#91;</span>34<span class=\"cite-bracket\">&#93;</span></a></sup></li>\\n<li><a href=\"/wiki/Adversarial_stylometry\" title=\"Adversarial stylometry\">Adversarial stylometry</a> may make use of summaries, if the detail lost is not major and the summary is sufficiently stylistically different to the input.<sup id=\"cite_ref-FOOTNOTEPotthastHagenStein201611-12_35-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEPotthastHagenStein201611-12-35\"><span class=\"cite-bracket\">&#91;</span>35<span class=\"cite-bracket\">&#93;</span></a></sup></li></ul>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"Evaluation\">Evaluation</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=19\" title=\"Edit section: Evaluation\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\\n</p><p>Evaluation can be intrinsic or extrinsic,<sup id=\"cite_ref-36\" class=\"reference\"><a href=\"#cite_note-36\"><span class=\"cite-bracket\">&#91;</span>36<span class=\"cite-bracket\">&#93;</span></a></sup> and inter-textual or intra-textual.<sup id=\"cite_ref-37\" class=\"reference\"><a href=\"#cite_note-37\"><span class=\"cite-bracket\">&#91;</span>37<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Intrinsic_versus_extrinsic\">Intrinsic versus extrinsic</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=20\" title=\"Edit section: Intrinsic versus extrinsic\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Intrinsic evaluation assesses the summaries directly, while extrinsic evaluation evaluates how the summarization system affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Inter-textual_versus_intra-textual\">Inter-textual versus intra-textual</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=21\" title=\"Edit section: Inter-textual versus intra-textual\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Intra-textual evaluation assess the output of a specific summarization system, while inter-textual evaluation focuses on contrastive analysis of outputs of several summarization systems.\\n</p><p>Human judgement often varies greatly in what it considers a \"good\" summary, so creating an automatic evaluation process is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive, as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning <a href=\"/wiki/Coherence_(linguistics)\" title=\"Coherence (linguistics)\">coherence</a> and coverage.\\n</p><p>The most common way to evaluate summaries is <a href=\"/wiki/ROUGE_(metric)\" title=\"ROUGE (metric)\">ROUGE</a> (Recall-Oriented Understudy for Gisting Evaluation). It is very common for summarization and translation systems in <a href=\"/wiki/NIST\" class=\"mw-redirect\" title=\"NIST\">NIST</a>\\'s Document Understanding Conferences.<a rel=\"nofollow\" class=\"external autonumber\" href=\"https://web.archive.org/web/20060408135021/http://haydn.isi.edu/ROUGE/\">[2]</a> ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references. It calculates <a href=\"/wiki/N-gram\" title=\"N-gram\">n-gram</a> overlaps between automatically generated summaries and previously written human summaries. It is recall-based to encourage inclusion of all important topics in summaries. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is the fraction of unigrams that appear in both the reference summary and the automatic summary out of all unigrams in the reference summary. If there are multiple reference summaries, their scores are averaged. A high level of overlap should indicate a high degree of shared concepts between the two summaries.\\n</p><p>ROUGE cannot determine if the result is coherent, that is if sentences flow together in a sensibly. High-order n-gram ROUGE measures help to some degree.\\n</p><p>Another unsolved problem is <a href=\"/wiki/Anaphora_(linguistics)\" title=\"Anaphora (linguistics)\">Anaphor resolution</a>. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.<sup id=\"cite_ref-38\" class=\"reference\"><a href=\"#cite_note-38\"><span class=\"cite-bracket\">&#91;</span>38<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Domain-specific_versus_domain-independent_summarization\">Domain-specific versus domain-independent summarization</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=22\" title=\"Edit section: Domain-specific versus domain-independent summarization\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Domain-independent summarization techniques apply sets of general features to identify information-rich text segments. Recent research focuses on domain-specific summarization using knowledge specific to the text\\'s domain, such as medical knowledge and ontologies for summarizing medical texts.<sup id=\"cite_ref-39\" class=\"reference\"><a href=\"#cite_note-39\"><span class=\"cite-bracket\">&#91;</span>39<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Qualitative\">Qualitative</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=23\" title=\"Edit section: Qualitative\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>The main drawback of the evaluation systems so far is that we need a reference summary (for some methods, more than one), to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be made to create corpora of texts and their corresponding summaries. Furthermore, some methods require manual annotation of the summaries (e.g. SCU in the Pyramid Method). Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\\n</p>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"History\">History</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=24\" title=\"Edit section: History\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>The first publication in the area dates back to 1957 <sup id=\"cite_ref-40\" class=\"reference\"><a href=\"#cite_note-40\"><span class=\"cite-bracket\">&#91;</span>40<span class=\"cite-bracket\">&#93;</span></a></sup> (<a href=\"/wiki/Hans_Peter_Luhn\" title=\"Hans Peter Luhn\">Hans Peter Luhn</a>), starting with a statistical technique. Research increased significantly in 2015. <a href=\"/wiki/Term_frequency%E2%80%93inverse_document_frequency\" class=\"mw-redirect\" title=\"Term frequency\\xe2\\x80\\x93inverse document frequency\">Term frequency\\xe2\\x80\\x93inverse document frequency</a> had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by <a href=\"/wiki/Latent_semantic_analysis\" title=\"Latent semantic analysis\">latent semantic analysis</a> (LSA) combined with <a href=\"/wiki/Non-negative_matrix_factorization\" title=\"Non-negative matrix factorization\">non-negative matrix factorization</a> (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020, the field was still very active and research is shifting towards abstractive summation and real-time summarization.<sup id=\"cite_ref-41\" class=\"reference\"><a href=\"#cite_note-41\"><span class=\"cite-bracket\">&#91;</span>41<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading3\"><h3 id=\"Recent_approaches\">Recent approaches</h3><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=25\" title=\"Edit section: Recent approaches\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<p>Recently the rise of <a href=\"/wiki/Transformer_(machine_learning_model)\" class=\"mw-redirect\" title=\"Transformer (machine learning model)\">transformer models</a> replacing more traditional <a href=\"/wiki/Rnn_(software)\" title=\"Rnn (software)\">RNN</a> (<a href=\"/wiki/LSTM\" class=\"mw-redirect\" title=\"LSTM\">LSTM</a>) have provided a flexibility in the mapping of text sequences to text sequences of a different type, which is well suited to automatic summarization. This includes models such as T5<sup id=\"cite_ref-42\" class=\"reference\"><a href=\"#cite_note-42\"><span class=\"cite-bracket\">&#91;</span>42<span class=\"cite-bracket\">&#93;</span></a></sup> and Pegasus.<sup id=\"cite_ref-43\" class=\"reference\"><a href=\"#cite_note-43\"><span class=\"cite-bracket\">&#91;</span>43<span class=\"cite-bracket\">&#93;</span></a></sup>\\n</p>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"See_also\">See also</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=26\" title=\"Edit section: See also\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<ul><li><a href=\"/wiki/Sentence_extraction\" title=\"Sentence extraction\">Sentence extraction</a></li>\\n<li><a href=\"/wiki/Text_mining\" title=\"Text mining\">Text mining</a></li>\\n<li><a href=\"/wiki/Multi-document_summarization\" title=\"Multi-document summarization\">Multi-document summarization</a></li></ul>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"References\">References</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=27\" title=\"Edit section: References\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<style data-mw-deduplicate=\"TemplateStyles:r1239543626\">.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class=\"reflist reflist-columns references-column-width reflist-columns-2\">\\n<ol class=\"references\">\\n<li id=\"cite_note-Torres2014-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Torres2014_1-0\">^</a></b></span> <span class=\"reference-text\"><style data-mw-deduplicate=\"TemplateStyles:r1238218222\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}</style><cite id=\"CITEREFTorres-Moreno,_Juan-Manuel2014\" class=\"citation book cs1\">Torres-Moreno, Juan-Manuel (1 October 2014). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.wiley.com/en-gb/Automatic+Text+Summarization-p-9781848216686\"><i>Automatic Text Summarization</i></a>. Wiley. pp.&#160;320\\xe2\\x80\\x93. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-848-21668-6\" title=\"Special:BookSources/978-1-848-21668-6\"><bdi>978-1-848-21668-6</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Automatic+Text+Summarization&amp;rft.pages=320-&amp;rft.pub=Wiley&amp;rft.date=2014-10-01&amp;rft.isbn=978-1-848-21668-6&amp;rft.au=Torres-Moreno%2C+Juan-Manuel&amp;rft_id=https%3A%2F%2Fwww.wiley.com%2Fen-gb%2FAutomatic%2BText%2BSummarization-p-9781848216686&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFPanTangDongMa2021\" class=\"citation journal cs1\">Pan, Xingjia; Tang, Fan; Dong, Weiming; Ma, Chongyang; Meng, Yiping; Huang, Feiyue; Lee, Tong-Yee; Xu, Changsheng (2021-04-01). \"Content-Based Visual Summarization for Image Collection\". <i>IEEE Transactions on Visualization and Computer Graphics</i>. <b>27</b> (4): <span class=\"nowrap\">2298\\xe2\\x80\\x93</span>2312. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2Ftvcg.2019.2948611\">10.1109/tvcg.2019.2948611</a>. <a href=\"/wiki/ISSN_(identifier)\" class=\"mw-redirect\" title=\"ISSN (identifier)\">ISSN</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://search.worldcat.org/issn/1077-2626\">1077-2626</a>. <a href=\"/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/31647438\">31647438</a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:204865221\">204865221</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Visualization+and+Computer+Graphics&amp;rft.atitle=Content-Based+Visual+Summarization+for+Image+Collection&amp;rft.volume=27&amp;rft.issue=4&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E2298-%3C%2Fspan%3E2312&amp;rft.date=2021-04-01&amp;rft.issn=1077-2626&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A204865221%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F31647438&amp;rft_id=info%3Adoi%2F10.1109%2Ftvcg.2019.2948611&amp;rft.aulast=Pan&amp;rft.aufirst=Xingjia&amp;rft.au=Tang%2C+Fan&amp;rft.au=Dong%2C+Weiming&amp;rft.au=Ma%2C+Chongyang&amp;rft.au=Meng%2C+Yiping&amp;rft.au=Huang%2C+Feiyue&amp;rft.au=Lee%2C+Tong-Yee&amp;rft.au=Xu%2C+Changsheng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation news cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.proquest.com/docview/1986931333\">\"WIPO PUBLISHES PATENT OF KT FOR \"IMAGE SUMMARIZATION SYSTEM AND METHOD\" (SOUTH KOREAN INVENTORS)\"</a>. <i>US Fed News Service</i>. January 10, 2018. <link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><a href=\"/wiki/ProQuest\" title=\"ProQuest\">ProQuest</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://www.proquest.com/docview/1986931333\">1986931333</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 22,</span> 2021</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=US+Fed+News+Service&amp;rft.atitle=WIPO+PUBLISHES+PATENT+OF+KT+FOR+%22IMAGE+SUMMARIZATION+SYSTEM+AND+METHOD%22+%28SOUTH+KOREAN+INVENTORS%29&amp;rft.date=2018-01-10&amp;rft_id=https%3A%2F%2Fwww.proquest.com%2Fdocview%2F1986931333&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFLi_TanYangqiu_SongShixia_LiuLexing_Xie2012\" class=\"citation journal cs1\">Li Tan; Yangqiu Song; <a href=\"/wiki/Shixia_Liu\" title=\"Shixia Liu\">Shixia Liu</a>; Lexing Xie (February 2012). \"ImageHive: Interactive Content-Aware Image Summarization\". <i>IEEE Computer Graphics and Applications</i>. <b>32</b> (1): <span class=\"nowrap\">46\\xe2\\x80\\x93</span>55. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2Fmcg.2011.89\">10.1109/mcg.2011.89</a>. <a href=\"/wiki/ISSN_(identifier)\" class=\"mw-redirect\" title=\"ISSN (identifier)\">ISSN</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://search.worldcat.org/issn/0272-1716\">0272-1716</a>. <a href=\"/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/24808292\">24808292</a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:7668289\">7668289</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Computer+Graphics+and+Applications&amp;rft.atitle=ImageHive%3A+Interactive+Content-Aware+Image+Summarization&amp;rft.volume=32&amp;rft.issue=1&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E46-%3C%2Fspan%3E55&amp;rft.date=2012-02&amp;rft.issn=0272-1716&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7668289%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F24808292&amp;rft_id=info%3Adoi%2F10.1109%2Fmcg.2011.89&amp;rft.au=Li+Tan&amp;rft.au=Yangqiu+Song&amp;rft.au=Shixia+Liu&amp;rft.au=Lexing+Xie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-PalPetrosino2012-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-PalPetrosino2012_5-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFSankar_K._PalAlfredo_PetrosinoLucia_Maddalena2012\" class=\"citation book cs1\">Sankar K. Pal; Alfredo Petrosino; Lucia Maddalena (25 January 2012). <a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/books?id=O0fNBQAAQBAJ&amp;q=video+surveillance+summarization&amp;pg=PA81\"><i>Handbook on Soft Computing for Video Surveillance</i></a>. CRC Press. pp.&#160;81\\xe2\\x80\\x93. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-4398-5685-7\" title=\"Special:BookSources/978-1-4398-5685-7\"><bdi>978-1-4398-5685-7</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Handbook+on+Soft+Computing+for+Video+Surveillance&amp;rft.pages=81-&amp;rft.pub=CRC+Press&amp;rft.date=2012-01-25&amp;rft.isbn=978-1-4398-5685-7&amp;rft.au=Sankar+K.+Pal&amp;rft.au=Alfredo+Petrosino&amp;rft.au=Lucia+Maddalena&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DO0fNBQAAQBAJ%26q%3Dvideo%2Bsurveillance%2Bsummarization%26pg%3DPA81&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-Elhamifar2012-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Elhamifar2012_6-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFElhamifarSapiroVidal2012\" class=\"citation book cs1\">Elhamifar, Ehsan; Sapiro, Guillermo; Vidal, Rene (2012). \"See all by looking at a few: Sparse modeling for finding representative objects\". <a rel=\"nofollow\" class=\"external text\" href=\"https://ieeexplore.ieee.org/document/6247852\"><i>2012 IEEE Conference on Computer Vision and Pattern Recognition</i></a>. IEEE. pp.&#160;<span class=\"nowrap\">1600\\xe2\\x80\\x93</span>1607. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FCVPR.2012.6247852\">10.1109/CVPR.2012.6247852</a>. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-4673-1228-8\" title=\"Special:BookSources/978-1-4673-1228-8\"><bdi>978-1-4673-1228-8</bdi></a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:5909301\">5909301</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">4 December</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=See+all+by+looking+at+a+few%3A+Sparse+modeling+for+finding+representative+objects&amp;rft.btitle=2012+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E1600-%3C%2Fspan%3E1607&amp;rft.pub=IEEE&amp;rft.date=2012&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5909301%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FCVPR.2012.6247852&amp;rft.isbn=978-1-4673-1228-8&amp;rft.aulast=Elhamifar&amp;rft.aufirst=Ehsan&amp;rft.au=Sapiro%2C+Guillermo&amp;rft.au=Vidal%2C+Rene&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F6247852&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-Mademlis2016-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Mademlis2016_7-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFMademlisTefasNikolaidisPitas2016\" class=\"citation journal cs1\">Mademlis, Ioannis; Tefas, Anastasios; Nikolaidis, Nikos; Pitas, Ioannis (2016). <a rel=\"nofollow\" class=\"external text\" href=\"https://research-information.bris.ac.uk/files/111433536/Ioannis_Pitas_Multimodal_Stereoscopic_Movie_Summarization_Conforming_to_Narrative_Characteristics.pdf\">\"Multimodal stereoscopic movie summarization conforming to narrative characteristics\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>IEEE Transactions on Image Processing</i>. <b>25</b> (12). IEEE: <span class=\"nowrap\">5828\\xe2\\x80\\x93</span>5840. <a href=\"/wiki/Bibcode_(identifier)\" class=\"mw-redirect\" title=\"Bibcode (identifier)\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2016ITIP...25.5828M\">2016ITIP...25.5828M</a>. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTIP.2016.2615289\">10.1109/TIP.2016.2615289</a>. <a href=\"/wiki/Hdl_(identifier)\" class=\"mw-redirect\" title=\"Hdl (identifier)\">hdl</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://hdl.handle.net/1983%2F2bcdd7a5-825f-4ac9-90ec-f2f538bfcb72\">1983/2bcdd7a5-825f-4ac9-90ec-f2f538bfcb72</a>. <a href=\"/wiki/PMID_(identifier)\" class=\"mw-redirect\" title=\"PMID (identifier)\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://pubmed.ncbi.nlm.nih.gov/28113502\">28113502</a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:18566122\">18566122</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">4 December</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Image+Processing&amp;rft.atitle=Multimodal+stereoscopic+movie+summarization+conforming+to+narrative+characteristics&amp;rft.volume=25&amp;rft.issue=12&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E5828-%3C%2Fspan%3E5840&amp;rft.date=2016&amp;rft_id=info%3Ahdl%2F1983%2F2bcdd7a5-825f-4ac9-90ec-f2f538bfcb72&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A18566122%23id-name%3DS2CID&amp;rft_id=info%3Abibcode%2F2016ITIP...25.5828M&amp;rft_id=info%3Apmid%2F28113502&amp;rft_id=info%3Adoi%2F10.1109%2FTIP.2016.2615289&amp;rft.aulast=Mademlis&amp;rft.aufirst=Ioannis&amp;rft.au=Tefas%2C+Anastasios&amp;rft.au=Nikolaidis%2C+Nikos&amp;rft.au=Pitas%2C+Ioannis&amp;rft_id=https%3A%2F%2Fresearch-information.bris.ac.uk%2Ffiles%2F111433536%2FIoannis_Pitas_Multimodal_Stereoscopic_Movie_Summarization_Conforming_to_Narrative_Characteristics.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-Mademlis2018-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Mademlis2018_8-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFMademlisTefasPitas2018\" class=\"citation journal cs1\">Mademlis, Ioannis; Tefas, Anastasios; Pitas, Ioannis (2018). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S0020025517311398\">\"A salient dictionary learning framework for activity video summarization via key-frame extraction\"</a>. <i>Information Sciences</i>. <b>432</b>. Elsevier: <span class=\"nowrap\">319\\xe2\\x80\\x93</span>331. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2Fj.ins.2017.12.020\">10.1016/j.ins.2017.12.020</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">4 December</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Sciences&amp;rft.atitle=A+salient+dictionary+learning+framework+for+activity+video+summarization+via+key-frame+extraction&amp;rft.volume=432&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E319-%3C%2Fspan%3E331&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ins.2017.12.020&amp;rft.aulast=Mademlis&amp;rft.aufirst=Ioannis&amp;rft.au=Tefas%2C+Anastasios&amp;rft.au=Pitas%2C+Ioannis&amp;rft_id=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fabs%2Fpii%2FS0020025517311398&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"http://ai.googleblog.com/2022/03/auto-generated-summaries-in-google-docs.html\">\"Auto-generated Summaries in Google Docs\"</a>. <i>Google AI Blog</i>. 23 March 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2022-04-03</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+AI+Blog&amp;rft.atitle=Auto-generated+Summaries+in+Google+Docs&amp;rft.date=2022-03-23&amp;rft_id=http%3A%2F%2Fai.googleblog.com%2F2022%2F03%2Fauto-generated-summaries-in-google-docs.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\">Richard Sutz, Peter Weverka. How to skim text. <a rel=\"nofollow\" class=\"external free\" href=\"https://www.dummies.com/education/language-arts/speed-reading/how-to-skim-text/\">https://www.dummies.com/education/language-arts/speed-reading/how-to-skim-text/</a> Accessed Dec 2019.</span>\\n</li>\\n<li id=\"cite_note-Afzal_et_al-11\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Afzal_et_al_11-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Afzal_et_al_11-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Afzal M, Alam F, Malik KM, Malik GM, <a rel=\"nofollow\" class=\"external text\" href=\"https://www.jmir.org/2020/10/e19810/\">Clinical Context-Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation</a>, J Med Internet Res 2020;22(10):e19810, DOI: 10.2196/19810, PMID 33095174</span>\\n</li>\\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFZhai2016\" class=\"citation book cs1\">Zhai, ChengXiang (2016). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.worldcat.org/oclc/957355971\"><i>Text data management and analysis&#160;: a practical introduction to information retrieval and text mining</i></a>. Sean Massung. [New York, NY]. p.&#160;321. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-970001-19-8\" title=\"Special:BookSources/978-1-970001-19-8\"><bdi>978-1-970001-19-8</bdi></a>. <a href=\"/wiki/OCLC_(identifier)\" class=\"mw-redirect\" title=\"OCLC (identifier)\">OCLC</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://search.worldcat.org/oclc/957355971\">957355971</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Text+data+management+and+analysis+%3A+a+practical+introduction+to+information+retrieval+and+text+mining&amp;rft.place=%5BNew+York%2C+NY%5D&amp;rft.pages=321&amp;rft.date=2016&amp;rft_id=info%3Aoclcnum%2F957355971&amp;rft.isbn=978-1-970001-19-8&amp;rft.aulast=Zhai&amp;rft.aufirst=ChengXiang&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F957355971&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: location missing publisher (<a href=\"/wiki/Category:CS1_maint:_location_missing_publisher\" title=\"Category:CS1 maint: location missing publisher\">link</a>)</span></span>\\n</li>\\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\">Jorge E. Camargo and Fabio A. Gonz\\xc3\\xa1lez. A Multi-class Kernel Alignment Method for Image Collection Summarization. In Proceedings of the 14th Iberoamerican Conference on Pattern Recognition: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications (CIARP \\'09), Eduardo Bayro-Corrochano and Jan-Olof Eklundh (Eds.). Springer-Verlag, Berlin, Heidelberg, 545-552.  <link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-642-10268-4_64\">10.1007/978-3-642-10268-4_64</a></span>\\n</li>\\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFAlrehamyWalker2018\" class=\"citation book cs1\">Alrehamy, Hassan H; Walker, Coral (2018). \"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\". <i>Advances in Computational Intelligence Systems</i>. Advances in Intelligent Systems and Computing. Vol.&#160;650. pp.&#160;<span class=\"nowrap\">222\\xe2\\x80\\x93</span>235. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-319-66939-7_19\">10.1007/978-3-319-66939-7_19</a>. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-3-319-66938-0\" title=\"Special:BookSources/978-3-319-66938-0\"><bdi>978-3-319-66938-0</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=SemCluster%3A+Unsupervised+Automatic+Keyphrase+Extraction+Using+Affinity+Propagation&amp;rft.btitle=Advances+in+Computational+Intelligence+Systems&amp;rft.series=Advances+in+Intelligent+Systems+and+Computing&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E222-%3C%2Fspan%3E235&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-66939-7_19&amp;rft.isbn=978-3-319-66938-0&amp;rft.aulast=Alrehamy&amp;rft.aufirst=Hassan+H&amp;rft.au=Walker%2C+Coral&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFTurney2002\" class=\"citation journal cs1\">Turney, Peter D (2002). \"Learning Algorithms for Keyphrase Extraction\". <i>Information Retrieval</i>. <b>2</b> (4): <span class=\"nowrap\">303\\xe2\\x80\\x93</span>336. <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/cs/0212020\">cs/0212020</a></span>. <a href=\"/wiki/Bibcode_(identifier)\" class=\"mw-redirect\" title=\"Bibcode (identifier)\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2002cs.......12020T\">2002cs.......12020T</a>. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1023%2FA%3A1009976227802\">10.1023/A:1009976227802</a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:7007323\">7007323</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+Retrieval&amp;rft.atitle=Learning+Algorithms+for+Keyphrase+Extraction&amp;rft.volume=2&amp;rft.issue=4&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E303-%3C%2Fspan%3E336&amp;rft.date=2002&amp;rft_id=info%3Aarxiv%2Fcs%2F0212020&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7007323%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1009976227802&amp;rft_id=info%3Abibcode%2F2002cs.......12020T&amp;rft.aulast=Turney&amp;rft.aufirst=Peter+D&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\">Rada Mihalcea and Paul Tarau, 2004: <i>TextRank: Bringing Order into Texts</i>, Department of Computer Science University of North Texas <link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20120617170501/http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf\">\"Archived copy\"</a> <span class=\"cs1-format\">(PDF)</span>. Archived from the original on 2012-06-17<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2012-07-20</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Archived+copy&amp;rft_id=http%3A%2F%2Facl.ldc.upenn.edu%2Facl2004%2Femnlp%2Fpdf%2FMihalcea.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_web\" title=\"Template:Cite web\">cite web</a>}}</code>:  CS1 maint: archived copy as title (<a href=\"/wiki/Category:CS1_maint:_archived_copy_as_title\" title=\"Category:CS1 maint: archived copy as title\">link</a>) CS1 maint: bot: original URL status unknown (<a href=\"/wiki/Category:CS1_maint:_bot:_original_URL_status_unknown\" title=\"Category:CS1 maint: bot: original URL status unknown\">link</a>)</span></span>\\n</li>\\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFYatskoStarikovButakov2010\" class=\"citation journal cs1\">Yatsko, V. A.; Starikov, M. S.; Butakov, A. V. (2010). \"Automatic genre recognition and adaptive text summarization\". <i>Automatic Documentation and Mathematical Linguistics</i>. <b>44</b> (3): <span class=\"nowrap\">111\\xe2\\x80\\x93</span>120. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.3103%2FS0005105510030027\">10.3103/S0005105510030027</a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:1586931\">1586931</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Automatic+Documentation+and+Mathematical+Linguistics&amp;rft.atitle=Automatic+genre+recognition+and+adaptive+text+summarization&amp;rft.volume=44&amp;rft.issue=3&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E111-%3C%2Fspan%3E120&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.3103%2FS0005105510030027&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1586931%23id-name%3DS2CID&amp;rft.aulast=Yatsko&amp;rft.aufirst=V.+A.&amp;rft.au=Starikov%2C+M.+S.&amp;rft.au=Butakov%2C+A.+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://yatsko.zohosites.com/universal-summarizer-unis.html\">UNIS (Universal Summarizer)</a></span>\\n</li>\\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\">G\\xc3\\xbcne\\xc5\\x9f Erkan and Dragomir R. Radev: <i>LexRank: Graph-based Lexical Centrality as Salience in Text Summarization <a rel=\"nofollow\" class=\"external autonumber\" href=\"https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html\">[1]</a></i></span>\\n</li>\\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\">\"<a rel=\"nofollow\" class=\"external text\" href=\"https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis\">Versatile question answering systems: seeing in synthesis</a>\", International Journal of Intelligent Information Database Systems, 5(2), 119-142, 2011.</span>\\n</li>\\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\">Carbonell, Jaime, and Jade Goldstein. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/jgc/publication/MMR_DiversityBased_Reranking_SIGIR_1998.pdf\">The use of MMR, diversity-based reranking for reordering documents and producing summaries</a>.\" Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1998.</span>\\n</li>\\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\">Zhu, Xiaojin, et al. \"<a rel=\"nofollow\" class=\"external text\" href=\"http://www.aclweb.org/anthology/N07-1013\">Improving Diversity in Ranking using Absorbing Random Walks</a>.\" HLT-NAACL. 2007.</span>\\n</li>\\n<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\">Hui Lin, Jeff Bilmes. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/1210.4871\">Learning mixtures of submodular shells with application to document summarization</a></span>\\n</li>\\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\">Alex Kulesza and Ben Taskar, <a rel=\"nofollow\" class=\"external text\" href=\"http://www.nowpublishers.com/article/DownloadSummary/MAL-044\">Determinantal point processes for machine learning</a>. Foundations and Trends in Machine Learning, December 2012.</span>\\n</li>\\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\">Nemhauser, George L., Laurence A. Wolsey, and Marshall L. Fisher. \"An analysis of approximations for maximizing submodular set functions\\xe2\\x80\\x94I.\" Mathematical Programming 14.1 (1978): 265-294.</span>\\n</li>\\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\">Hui Lin, Jeff Bilmes. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/1210.4871\">Learning mixtures of submodular shells with application to document summarization</a>\", UAI, 2012</span>\\n</li>\\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\">Hui Lin, Jeff Bilmes. \"<a rel=\"nofollow\" class=\"external text\" href=\"http://www.aclweb.org/anthology/P11-1052\">A Class of Submodular Functions for Document Summarization</a>\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011</span>\\n</li>\\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\">Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, <a rel=\"nofollow\" class=\"external text\" href=\"http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf\">Learning Mixtures of Submodular Functions for Image Collection Summarization</a>, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.</span>\\n</li>\\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\">Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, <a rel=\"nofollow\" class=\"external text\" href=\"http://www.aclweb.org/anthology/P15-1054\">Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures</a>, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015</span>\\n</li>\\n<li id=\"cite_note-30\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-30\">^</a></b></span> <span class=\"reference-text\">Kai Wei, Rishabh Iyer, and Jeff Bilmes, <a rel=\"nofollow\" class=\"external text\" href=\"http://www.jmlr.org/proceedings/papers/v37/wei15.pdf\">Submodularity in Data Subset Selection and Active Learning</a> <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20170313220928/http://jmlr.org/proceedings/papers/v37/wei15.pdf\">Archived</a> 2017-03-13 at the <a href=\"/wiki/Wayback_Machine\" title=\"Wayback Machine\">Wayback Machine</a>, To Appear In Proc. International Conference on Machine Learning (ICML), Lille, France, June - 2015</span>\\n</li>\\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.reddit.com/user/autotldr\">\"overview for autotldr\"</a>. <i>reddit</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">9 February</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=reddit&amp;rft.atitle=overview+for+autotldr&amp;rft_id=https%3A%2F%2Fwww.reddit.com%2Fuser%2Fautotldr&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFSquire2016\" class=\"citation book cs1\"><a href=\"/wiki/Megan_Squire\" title=\"Megan Squire\">Squire, Megan</a> (2016-08-29). <a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/books?id=_qXWDQAAQBAJ&amp;pg=PA185\"><i>Mastering Data Mining with Python \\xe2\\x80\\x93 Find patterns hidden in your data</i></a>. Packt Publishing Ltd. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/9781785885914\" title=\"Special:BookSources/9781785885914\"><bdi>9781785885914</bdi></a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">9 February</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mastering+Data+Mining+with+Python+%E2%80%93+Find+patterns+hidden+in+your+data&amp;rft.pub=Packt+Publishing+Ltd&amp;rft.date=2016-08-29&amp;rft.isbn=9781785885914&amp;rft.aulast=Squire&amp;rft.aufirst=Megan&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D_qXWDQAAQBAJ%26pg%3DPA185&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.lifewire.com/what-is-tldr-2483633\">\"What Is \\'TLDR\\'?\"</a>. <i>Lifewire</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">9 February</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Lifewire&amp;rft.atitle=What+Is+%27TLDR%27%3F&amp;rft_id=https%3A%2F%2Fwww.lifewire.com%2Fwhat-is-tldr-2483633&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ibtimes.com/what-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704\">\"What Does TL;DR Mean? AMA? TIL? Glossary Of Reddit Terms And Abbreviations\"</a>. <i>International Business Times</i>. 29 March 2012<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">9 February</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=International+Business+Times&amp;rft.atitle=What+Does+TL%3BDR+Mean%3F+AMA%3F+TIL%3F+Glossary+Of+Reddit+Terms+And+Abbreviations&amp;rft.date=2012-03-29&amp;rft_id=http%3A%2F%2Fwww.ibtimes.com%2Fwhat-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-FOOTNOTEPotthastHagenStein201611-12-35\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEPotthastHagenStein201611-12_35-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFPotthastHagenStein2016\">Potthast, Hagen &amp; Stein 2016</a>, p.&#160;11-12.</span>\\n</li>\\n<li id=\"cite_note-36\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-36\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings2/sum-mani.pdf\">Mani, I. Summarization evaluation: an overview</a></span>\\n</li>\\n<li id=\"cite_note-37\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-37\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFYatskoVishnyakov2007\" class=\"citation journal cs1\">Yatsko, V. A.; Vishnyakov, T. N. (2007). \"A method for evaluating modern systems of automatic text summarization\". <i>Automatic Documentation and Mathematical Linguistics</i>. <b>41</b> (3): <span class=\"nowrap\">93\\xe2\\x80\\x93</span>103. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.3103%2FS0005105507030041\">10.3103/S0005105507030041</a>. <a href=\"/wiki/S2CID_(identifier)\" class=\"mw-redirect\" title=\"S2CID (identifier)\">S2CID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://api.semanticscholar.org/CorpusID:7853204\">7853204</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Automatic+Documentation+and+Mathematical+Linguistics&amp;rft.atitle=A+method+for+evaluating+modern+systems+of+automatic+text+summarization&amp;rft.volume=41&amp;rft.issue=3&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E93-%3C%2Fspan%3E103&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.3103%2FS0005105507030041&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7853204%23id-name%3DS2CID&amp;rft.aulast=Yatsko&amp;rft.aufirst=V.+A.&amp;rft.au=Vishnyakov%2C+T.+N.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-38\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-38\">^</a></b></span> <span class=\"reference-text\">Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, <a rel=\"nofollow\" class=\"external text\" href=\"http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf\">Learning Mixtures of Submodular Functions for Image Collection Summarization</a>, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014. (PDF)</span>\\n</li>\\n<li id=\"cite_note-39\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-39\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFSarkerMollaParis2013\" class=\"citation book cs1\">Sarker, Abeed; Molla, Diego; Paris, Cecile (2013). \"An Approach for Query-Focused Text Summarisation for Evidence Based Medicine\". <i>Artificial Intelligence in Medicine</i>. Lecture Notes in Computer Science. Vol.&#160;7885. pp.&#160;<span class=\"nowrap\">295\\xe2\\x80\\x93</span>304. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-642-38326-7_41\">10.1007/978-3-642-38326-7_41</a>. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-3-642-38325-0\" title=\"Special:BookSources/978-3-642-38325-0\"><bdi>978-3-642-38325-0</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=An+Approach+for+Query-Focused+Text+Summarisation+for+Evidence+Based+Medicine&amp;rft.btitle=Artificial+Intelligence+in+Medicine&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E295-%3C%2Fspan%3E304&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-38326-7_41&amp;rft.isbn=978-3-642-38325-0&amp;rft.aulast=Sarker&amp;rft.aufirst=Abeed&amp;rft.au=Molla%2C+Diego&amp;rft.au=Paris%2C+Cecile&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-40\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-40\">^</a></b></span> <span class=\"reference-text\">Luhn, Hans Peter (1957). \"A Statistical Approach to Mechanized Encoding and Searching of Literary Information\" (PDF). IBM Journal of Research and Development. 1 (4): 309\\xe2\\x80\\x93317. doi:10.1147/rd.14.0309.</span>\\n</li>\\n<li id=\"cite_note-41\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-41\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFWidyassariRustadShidikNoersasongko2020\" class=\"citation journal cs1\">Widyassari, Adhika Pramita; Rustad, Supriadi; Shidik, Guruh Fajar; Noersasongko, Edi; Syukur, Abdul; Affandy, Affandy; Setiadi, De Rosal Ignatius Moses (2020-05-20). <a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2Fj.jksuci.2020.05.006\">\"Review of automatic text summarization techniques &amp; methods\"</a>. <i>Journal of King Saud University - Computer and Information Sciences</i>. <b>34</b> (4): <span class=\"nowrap\">1029\\xe2\\x80\\x93</span>1046. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1016%2Fj.jksuci.2020.05.006\">10.1016/j.jksuci.2020.05.006</a></span>. <a href=\"/wiki/ISSN_(identifier)\" class=\"mw-redirect\" title=\"ISSN (identifier)\">ISSN</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"https://search.worldcat.org/issn/1319-1578\">1319-1578</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+King+Saud+University+-+Computer+and+Information+Sciences&amp;rft.atitle=Review+of+automatic+text+summarization+techniques+%26+methods&amp;rft.volume=34&amp;rft.issue=4&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E1029-%3C%2Fspan%3E1046&amp;rft.date=2020-05-20&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jksuci.2020.05.006&amp;rft.issn=1319-1578&amp;rft.aulast=Widyassari&amp;rft.aufirst=Adhika+Pramita&amp;rft.au=Rustad%2C+Supriadi&amp;rft.au=Shidik%2C+Guruh+Fajar&amp;rft.au=Noersasongko%2C+Edi&amp;rft.au=Syukur%2C+Abdul&amp;rft.au=Affandy%2C+Affandy&amp;rft.au=Setiadi%2C+De+Rosal+Ignatius+Moses&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.jksuci.2020.05.006&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-42\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-42\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\">\"Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer\"</a>. <i>Google AI Blog</i>. 24 February 2020<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2022-04-03</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+AI+Blog&amp;rft.atitle=Exploring+Transfer+Learning+with+T5%3A+the+Text-To-Text+Transfer+Transformer&amp;rft.date=2020-02-24&amp;rft_id=http%3A%2F%2Fai.googleblog.com%2F2020%2F02%2Fexploring-transfer-learning-with-t5.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></span>\\n</li>\\n<li id=\"cite_note-43\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-43\">^</a></b></span> <span class=\"reference-text\">Zhang, J., Zhao, Y., Saleh, M., &amp; Liu, P. (2020, November). Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning (pp. 11328-11339). PMLR.</span>\\n</li>\\n</ol></div>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"Works_cited\">Works cited</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=28\" title=\"Edit section: Works cited\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<ul><li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFPotthastHagenStein2016\" class=\"citation conference cs1\">Potthast, Martin; Hagen, Matthias; Stein, Benno (2016). <a rel=\"nofollow\" class=\"external text\" href=\"https://ceur-ws.org/Vol-1609/16090716.pdf\"><i>Author Obfuscation: Attacking the State of the Art in Authorship Verification</i></a> <span class=\"cs1-format\">(PDF)</span>. Conference and Labs of the Evaluation Forum.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Author+Obfuscation%3A+Attacking+the+State+of+the+Art+in+Authorship+Verification&amp;rft.date=2016&amp;rft.aulast=Potthast&amp;rft.aufirst=Martin&amp;rft.au=Hagen%2C+Matthias&amp;rft.au=Stein%2C+Benno&amp;rft_id=https%3A%2F%2Fceur-ws.org%2FVol-1609%2F16090716.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li></ul>\\n<div class=\"mw-heading mw-heading2\"><h2 id=\"Further_reading\">Further reading</h2><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=29\" title=\"Edit section: Further reading\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></div>\\n<ul><li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFHercules2003\" class=\"citation book cs1\">Hercules, Dalianis (2003). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/277288103\"><i>Porting and evaluation of automatic summarization</i></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Porting+and+evaluation+of+automatic+summarization&amp;rft.date=2003&amp;rft.aulast=Hercules&amp;rft.aufirst=Dalianis&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F277288103&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFRoxana2002\" class=\"citation book cs1\">Roxana, Angheluta (2002). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/2553088\"><i>The Use of Topic Segmentation for Automatic Summarization</i></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Use+of+Topic+Segmentation+for+Automatic+Summarization&amp;rft.date=2002&amp;rft.aulast=Roxana&amp;rft.aufirst=Angheluta&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F2553088&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFAnne2004\" class=\"citation book cs1\">Anne, Buist (2004). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20210123014007/http://www.cs.ru.nl/~kraaijw/pubs/Biblio/papers/meeting_sum_tno.pdf\"><i>Automatic Summarization of Meeting Data: A Feasibility Study</i></a> <span class=\"cs1-format\">(PDF)</span>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"https://www.cs.ru.nl/~kraaijw/pubs/Biblio/papers/meeting_sum_tno.pdf\">the original</a> <span class=\"cs1-format\">(PDF)</span> on 2021-01-23<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2020-07-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Automatic+Summarization+of+Meeting+Data%3A+A+Feasibility+Study&amp;rft.date=2004&amp;rft.aulast=Anne&amp;rft.aufirst=Buist&amp;rft_id=https%3A%2F%2Fwww.cs.ru.nl%2F~kraaijw%2Fpubs%2FBiblio%2Fpapers%2Fmeeting_sum_tno.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFAnnie2009\" class=\"citation book cs1\">Annie, Louis (2009). <a rel=\"nofollow\" class=\"external text\" href=\"https://repository.upenn.edu/cgi/viewcontent.cgi?article=1762&amp;context=cis_papers\"><i>Performance Confidence Estimation for Automatic Summarization</i></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Performance+Confidence+Estimation+for+Automatic+Summarization&amp;rft.date=2009&amp;rft.aulast=Annie&amp;rft.aufirst=Louis&amp;rft_id=https%3A%2F%2Frepository.upenn.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1762%26context%3Dcis_papers&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFElena2009\" class=\"citation book cs1\">Elena, Lloret and Manuel, Palomar (2009). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20181003061926/http://www.informatica.si/ojs-2.4.3/index.php/informatica/article/download/273/269\"><i>Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation</i></a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://www.informatica.si/ojs-2.4.3/index.php/informatica/article/download/273/269\">the original</a> on 2018-10-03<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2018-10-03</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Challenging+Issues+of+Automatic+Summarization%3A+Relevance+Detection+and+Quality-based+Evaluation&amp;rft.date=2009&amp;rft.aulast=Elena&amp;rft.aufirst=Lloret+and+Manuel%2C+Palomar&amp;rft_id=http%3A%2F%2Fwww.informatica.si%2Fojs-2.4.3%2Findex.php%2Finformatica%2Farticle%2Fdownload%2F273%2F269&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">link</a>)</span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFAndrew2007\" class=\"citation book cs1\">Andrew, Goldberg (2007). <i>Automatic Summarization</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Automatic+Summarization&amp;rft.date=2007&amp;rft.aulast=Andrew&amp;rft.aufirst=Goldberg&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFAlrehamy2018\" class=\"citation book cs1\">Alrehamy, Hassan (2018). \"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\". <i>Advances in Computational Intelligence Systems</i>. Advances in Intelligent Systems and Computing. Vol.&#160;650. pp.&#160;<span class=\"nowrap\">222\\xe2\\x80\\x93</span>235. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-319-66939-7_19\">10.1007/978-3-319-66939-7_19</a>. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-3-319-66938-0\" title=\"Special:BookSources/978-3-319-66938-0\"><bdi>978-3-319-66938-0</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=SemCluster%3A+Unsupervised+Automatic+Keyphrase+Extraction+Using+Affinity+Propagation&amp;rft.btitle=Advances+in+Computational+Intelligence+Systems&amp;rft.series=Advances+in+Intelligent+Systems+and+Computing&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E222-%3C%2Fspan%3E235&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-66939-7_19&amp;rft.isbn=978-3-319-66938-0&amp;rft.aulast=Alrehamy&amp;rft.aufirst=Hassan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFEndres-Niggemeyer1998\" class=\"citation book cs1\">Endres-Niggemeyer, Brigitte (1998). <a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/springer_10.1007-978-3-642-72025-3\"><i>Summarizing Information</i></a>. Springer. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-3-540-63735-6\" title=\"Special:BookSources/978-3-540-63735-6\"><bdi>978-3-540-63735-6</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Summarizing+Information&amp;rft.pub=Springer&amp;rft.date=1998&amp;rft.isbn=978-3-540-63735-6&amp;rft.aulast=Endres-Niggemeyer&amp;rft.aufirst=Brigitte&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fspringer_10.1007-978-3-642-72025-3&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFMarcu2000\" class=\"citation book cs1\">Marcu, Daniel (2000). <i>The Theory and Practice of Discourse Parsing and Summarization</i>. MIT Press. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-0-262-13372-2\" title=\"Special:BookSources/978-0-262-13372-2\"><bdi>978-0-262-13372-2</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Theory+and+Practice+of+Discourse+Parsing+and+Summarization&amp;rft.pub=MIT+Press&amp;rft.date=2000&amp;rft.isbn=978-0-262-13372-2&amp;rft.aulast=Marcu&amp;rft.aufirst=Daniel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFMani2001\" class=\"citation book cs1\">Mani, Inderjeet (2001). <i>Automatic Summarization</i>. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-58811-060-2\" title=\"Special:BookSources/978-1-58811-060-2\"><bdi>978-1-58811-060-2</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Automatic+Summarization&amp;rft.date=2001&amp;rft.isbn=978-1-58811-060-2&amp;rft.aulast=Mani&amp;rft.aufirst=Inderjeet&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span></li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFHuff2010\" class=\"citation book cs1\">Huff, Jason (2010). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.jason-huff.com/projects/autosummarize/\"><i>AutoSummarize</i></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=AutoSummarize&amp;rft.date=2010&amp;rft.aulast=Huff&amp;rft.aufirst=Jason&amp;rft_id=http%3A%2F%2Fwww.jason-huff.com%2Fprojects%2Fautosummarize%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span>, Conceptual artwork using automatic summarization software in Microsoft Word 2008.</li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFLehmam2010\" class=\"citation book cs1\">Lehmam, Abderrafih (2010). <a rel=\"nofollow\" class=\"external text\" href=\"http://portal.acm.org/citation.cfm?id=1937055.1937111&amp;coll=DL&amp;dl=GUIDE&amp;CFID=23185814&amp;CFTOKEN=40272014/\"><i>Essential summarizer: innovative automatic text summarization software in twenty languages - ACM Digital Library</i></a>. Riao \\'10. pp.&#160;<span class=\"nowrap\">216\\xe2\\x80\\x93</span>217.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Essential+summarizer%3A+innovative+automatic+text+summarization+software+in+twenty+languages+-+ACM+Digital+Library&amp;rft.series=Riao+%2710&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E216-%3C%2Fspan%3E217&amp;rft.date=2010&amp;rft.aulast=Lehmam&amp;rft.aufirst=Abderrafih&amp;rft_id=http%3A%2F%2Fportal.acm.org%2Fcitation.cfm%3Fid%3D1937055.1937111%26coll%3DDL%26dl%3DGUIDE%26CFID%3D23185814%26CFTOKEN%3D40272014%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span>, Published in Proceeding RIAO\\'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France</li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFXiaojin2007\" class=\"citation book cs1\">Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). <a rel=\"nofollow\" class=\"external text\" href=\"http://pages.cs.wisc.edu/~jerryzhu/pub/grasshopper.pdf\"><i>Improving diversity in ranking using absorbing random walks</i></a> <span class=\"cs1-format\">(PDF)</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Improving+diversity+in+ranking+using+absorbing+random+walks&amp;rft.date=2007&amp;rft.aulast=Xiaojin&amp;rft.aufirst=Zhu%2C+Andrew+Goldberg%2C+Jurgen+Van+Gael%2C+and+David+Andrzejewski&amp;rft_id=http%3A%2F%2Fpages.cs.wisc.edu%2F~jerryzhu%2Fpub%2Fgrasshopper.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">link</a>)</span>, The GRASSHOPPER algorithm</li>\\n<li><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1238218222\" /><cite id=\"CITEREFMiranda-Jim\\xc3\\xa9nez2013\" class=\"citation book cs1\">Miranda-Jim\\xc3\\xa9nez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). \"Summarizing Conceptual Graphs for Automatic Summarization Task\". <i>Conceptual Structures for STEM Research and Education</i>. Lecture Notes in Computer Science. Vol.&#160;7735. pp.&#160;<span class=\"nowrap\">245\\xe2\\x80\\x93</span>253. <a href=\"/wiki/Doi_(identifier)\" class=\"mw-redirect\" title=\"Doi (identifier)\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2F978-3-642-35786-2_18\">10.1007/978-3-642-35786-2_18</a>. <a href=\"/wiki/ISBN_(identifier)\" class=\"mw-redirect\" title=\"ISBN (identifier)\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-3-642-35785-5\" title=\"Special:BookSources/978-3-642-35785-5\"><bdi>978-3-642-35785-5</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Summarizing+Conceptual+Graphs+for+Automatic+Summarization+Task&amp;rft.btitle=Conceptual+Structures+for+STEM+Research+and+Education&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=%3Cspan+class%3D%22nowrap%22%3E245-%3C%2Fspan%3E253&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-35786-2_18&amp;rft.isbn=978-3-642-35785-5&amp;rft.aulast=Miranda-Jim%C3%A9nez&amp;rft.aufirst=Sabino%2C+Gelbukh%2C+Alexander%2C+and+Sidorov%2C+Grigori&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_book\" title=\"Template:Cite book\">cite book</a>}}</code>:  CS1 maint: multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">link</a>)</span>, Conceptual Structures for STEM Research and Education.</li></ul>\\n<div class=\"navbox-styles\"><style data-mw-deduplicate=\"TemplateStyles:r1129693374\">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" \\xc2\\xb7 \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\\\a0 \"}</style><style data-mw-deduplicate=\"TemplateStyles:r1236075235\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}</style></div><div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Natural_language_processing454\" style=\"padding:3px\"><table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1129693374\" /><style data-mw-deduplicate=\"TemplateStyles:r1239400231\">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}</style><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Natural_language_processing\" title=\"Template:Natural language processing\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Natural_language_processing\" title=\"Template talk:Natural language processing\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a href=\"/wiki/Special:EditPage/Template:Natural_language_processing\" title=\"Special:EditPage/Template:Natural language processing\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Natural_language_processing454\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">General terms</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/AI-complete\" title=\"AI-complete\">AI-complete</a></li>\\n<li><a href=\"/wiki/Bag-of-words_model\" title=\"Bag-of-words model\">Bag-of-words</a></li>\\n<li><a href=\"/wiki/N-gram\" title=\"N-gram\">n-gram</a>\\n<ul><li><a href=\"/wiki/Bigram\" title=\"Bigram\">Bigram</a></li>\\n<li><a href=\"/wiki/Trigram\" title=\"Trigram\">Trigram</a></li></ul></li>\\n<li><a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">Computational linguistics</a></li>\\n<li><a href=\"/wiki/Natural_language_understanding\" title=\"Natural language understanding\">Natural language understanding</a></li>\\n<li><a href=\"/wiki/Stop_word\" title=\"Stop word\">Stop words</a></li>\\n<li><a href=\"/wiki/Text_processing\" title=\"Text processing\">Text processing</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Text_mining\" title=\"Text mining\">Text analysis</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Argument_mining\" title=\"Argument mining\">Argument mining</a></li>\\n<li><a href=\"/wiki/Collocation_extraction\" title=\"Collocation extraction\">Collocation extraction</a></li>\\n<li><a href=\"/wiki/Concept_mining\" title=\"Concept mining\">Concept mining</a></li>\\n<li><a href=\"/wiki/Coreference#Coreference_resolution\" title=\"Coreference\">Coreference resolution</a></li>\\n<li><a href=\"/wiki/Deep_linguistic_processing\" title=\"Deep linguistic processing\">Deep linguistic processing</a></li>\\n<li><a href=\"/wiki/Distant_reading\" title=\"Distant reading\">Distant reading</a></li>\\n<li><a href=\"/wiki/Information_extraction\" title=\"Information extraction\">Information extraction</a></li>\\n<li><a href=\"/wiki/Named-entity_recognition\" title=\"Named-entity recognition\">Named-entity recognition</a></li>\\n<li><a href=\"/wiki/Ontology_learning\" title=\"Ontology learning\">Ontology learning</a></li>\\n<li><a href=\"/wiki/Parsing\" title=\"Parsing\">Parsing</a>\\n<ul><li><a href=\"/wiki/Semantic_parsing\" title=\"Semantic parsing\">Semantic parsing</a></li>\\n<li><a href=\"/wiki/Syntactic_parsing_(computational_linguistics)\" title=\"Syntactic parsing (computational linguistics)\">Syntactic parsing</a></li></ul></li>\\n<li><a href=\"/wiki/Part-of-speech_tagging\" title=\"Part-of-speech tagging\">Part-of-speech tagging</a></li>\\n<li><a href=\"/wiki/Semantic_analysis_(machine_learning)\" title=\"Semantic analysis (machine learning)\">Semantic analysis</a></li>\\n<li><a href=\"/wiki/Semantic_role_labeling\" title=\"Semantic role labeling\">Semantic role labeling</a></li>\\n<li><a href=\"/wiki/Semantic_decomposition_(natural_language_processing)\" title=\"Semantic decomposition (natural language processing)\">Semantic decomposition</a></li>\\n<li><a href=\"/wiki/Semantic_similarity\" title=\"Semantic similarity\">Semantic similarity</a></li>\\n<li><a href=\"/wiki/Sentiment_analysis\" title=\"Sentiment analysis\">Sentiment analysis</a></li></ul>\\n<ul><li><a href=\"/wiki/Terminology_extraction\" title=\"Terminology extraction\">Terminology extraction</a></li>\\n<li><a href=\"/wiki/Text_mining\" title=\"Text mining\">Text mining</a></li>\\n<li><a href=\"/wiki/Textual_entailment\" title=\"Textual entailment\">Textual entailment</a></li>\\n<li><a href=\"/wiki/Truecasing\" title=\"Truecasing\">Truecasing</a></li>\\n<li><a href=\"/wiki/Word-sense_disambiguation\" title=\"Word-sense disambiguation\">Word-sense disambiguation</a></li>\\n<li><a href=\"/wiki/Word-sense_induction\" title=\"Word-sense induction\">Word-sense induction</a></li></ul>\\n</div><table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"><tbody><tr><th id=\"Text_segmentation21\" scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Text_segmentation\" title=\"Text segmentation\">Text segmentation</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Compound-term_processing\" title=\"Compound-term processing\">Compound-term processing</a></li>\\n<li><a href=\"/wiki/Lemmatisation\" class=\"mw-redirect\" title=\"Lemmatisation\">Lemmatisation</a></li>\\n<li><a href=\"/wiki/Lexical_analysis\" title=\"Lexical analysis\">Lexical analysis</a></li>\\n<li><a href=\"/wiki/Shallow_parsing\" title=\"Shallow parsing\">Text chunking</a></li>\\n<li><a href=\"/wiki/Stemming\" title=\"Stemming\">Stemming</a></li>\\n<li><a href=\"/wiki/Sentence_boundary_disambiguation\" title=\"Sentence boundary disambiguation\">Sentence segmentation</a></li>\\n<li><a href=\"/wiki/Word#Word_boundaries\" title=\"Word\">Word segmentation</a></li></ul>\\n</div></td></tr></tbody></table><div>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a class=\"mw-selflink selflink\">Automatic summarization</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Multi-document_summarization\" title=\"Multi-document summarization\">Multi-document summarization</a></li>\\n<li><a href=\"/wiki/Sentence_extraction\" title=\"Sentence extraction\">Sentence extraction</a></li>\\n<li><a href=\"/wiki/Text_simplification\" title=\"Text simplification\">Text simplification</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Machine_translation\" title=\"Machine translation\">Machine translation</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Computer-assisted_translation\" title=\"Computer-assisted translation\">Computer-assisted</a></li>\\n<li><a href=\"/wiki/Example-based_machine_translation\" title=\"Example-based machine translation\">Example-based</a></li>\\n<li><a href=\"/wiki/Rule-based_machine_translation\" title=\"Rule-based machine translation\">Rule-based</a></li>\\n<li><a href=\"/wiki/Statistical_machine_translation\" title=\"Statistical machine translation\">Statistical</a></li>\\n<li><a href=\"/wiki/Transfer-based_machine_translation\" title=\"Transfer-based machine translation\">Transfer-based</a></li>\\n<li><a href=\"/wiki/Neural_machine_translation\" title=\"Neural machine translation\">Neural</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Distributional_semantics\" title=\"Distributional semantics\">Distributional semantics</a> models</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/BERT_(language_model)\" title=\"BERT (language model)\">BERT</a></li>\\n<li><a href=\"/wiki/Document-term_matrix\" title=\"Document-term matrix\">Document-term matrix</a></li>\\n<li><a href=\"/wiki/Explicit_semantic_analysis\" title=\"Explicit semantic analysis\">Explicit semantic analysis</a></li>\\n<li><a href=\"/wiki/FastText\" title=\"FastText\">fastText</a></li>\\n<li><a href=\"/wiki/GloVe\" title=\"GloVe\">GloVe</a></li>\\n<li><a href=\"/wiki/Language_model\" title=\"Language model\">Language model</a> (<a href=\"/wiki/Large_language_model\" title=\"Large language model\">large</a>)</li>\\n<li><a href=\"/wiki/Latent_semantic_analysis\" title=\"Latent semantic analysis\">Latent semantic analysis</a></li>\\n<li><a href=\"/wiki/Seq2seq\" title=\"Seq2seq\">Seq2seq</a></li>\\n<li><a href=\"/wiki/Word_embedding\" title=\"Word embedding\">Word embedding</a></li>\\n<li><a href=\"/wiki/Word2vec\" title=\"Word2vec\">Word2vec</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Language_resource\" title=\"Language resource\">Language resources</a>,<br />datasets and corpora</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\"></div><table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"><tbody><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Types and<br />standards</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Corpus_linguistics\" title=\"Corpus linguistics\">Corpus linguistics</a></li>\\n<li><a href=\"/wiki/Lexical_resource\" title=\"Lexical resource\">Lexical resource</a></li>\\n<li><a href=\"/wiki/Linguistic_Linked_Open_Data\" title=\"Linguistic Linked Open Data\">Linguistic Linked Open Data</a></li>\\n<li><a href=\"/wiki/Machine-readable_dictionary\" title=\"Machine-readable dictionary\">Machine-readable dictionary</a></li>\\n<li><a href=\"/wiki/Parallel_text\" title=\"Parallel text\">Parallel text</a></li>\\n<li><a href=\"/wiki/PropBank\" title=\"PropBank\">PropBank</a></li>\\n<li><a href=\"/wiki/Semantic_network\" title=\"Semantic network\">Semantic network</a></li>\\n<li><a href=\"/wiki/Simple_Knowledge_Organization_System\" title=\"Simple Knowledge Organization System\">Simple Knowledge Organization System</a></li>\\n<li><a href=\"/wiki/Speech_corpus\" title=\"Speech corpus\">Speech corpus</a></li>\\n<li><a href=\"/wiki/Text_corpus\" title=\"Text corpus\">Text corpus</a></li>\\n<li><a href=\"/wiki/Thesaurus_(information_retrieval)\" title=\"Thesaurus (information retrieval)\">Thesaurus (information retrieval)</a></li>\\n<li><a href=\"/wiki/Treebank\" title=\"Treebank\">Treebank</a></li>\\n<li><a href=\"/wiki/Universal_Dependencies\" title=\"Universal Dependencies\">Universal Dependencies</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Data</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/BabelNet\" title=\"BabelNet\">BabelNet</a></li>\\n<li><a href=\"/wiki/Bank_of_English\" title=\"Bank of English\">Bank of English</a></li>\\n<li><a href=\"/wiki/DBpedia\" title=\"DBpedia\">DBpedia</a></li>\\n<li><a href=\"/wiki/FrameNet\" title=\"FrameNet\">FrameNet</a></li>\\n<li><a href=\"/wiki/Google_Ngram_Viewer\" class=\"mw-redirect\" title=\"Google Ngram Viewer\">Google Ngram Viewer</a></li>\\n<li><a href=\"/wiki/UBY\" title=\"UBY\">UBY</a></li>\\n<li><a href=\"/wiki/WordNet\" title=\"WordNet\">WordNet</a></li>\\n<li><a href=\"/wiki/Wikidata\" title=\"Wikidata\">Wikidata</a></li></ul>\\n</div></td></tr></tbody></table><div></div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Automatic_identification_and_data_capture\" title=\"Automatic identification and data capture\">Automatic identification<br />and data capture</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></li>\\n<li><a href=\"/wiki/Speech_segmentation\" title=\"Speech segmentation\">Speech segmentation</a></li>\\n<li><a href=\"/wiki/Speech_synthesis\" title=\"Speech synthesis\">Speech synthesis</a></li>\\n<li><a href=\"/wiki/Natural_language_generation\" title=\"Natural language generation\">Natural language generation</a></li>\\n<li><a href=\"/wiki/Optical_character_recognition\" title=\"Optical character recognition\">Optical character recognition</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Topic_model\" title=\"Topic model\">Topic model</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Document_classification\" title=\"Document classification\">Document classification</a></li>\\n<li><a href=\"/wiki/Latent_Dirichlet_allocation\" title=\"Latent Dirichlet allocation\">Latent Dirichlet allocation</a></li>\\n<li><a href=\"/wiki/Pachinko_allocation\" title=\"Pachinko allocation\">Pachinko allocation</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Computer-assisted_reviewing\" title=\"Computer-assisted reviewing\">Computer-assisted<br />reviewing</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Automated_essay_scoring\" title=\"Automated essay scoring\">Automated essay scoring</a></li>\\n<li><a href=\"/wiki/Concordancer\" title=\"Concordancer\">Concordancer</a></li>\\n<li><a href=\"/wiki/Grammar_checker\" title=\"Grammar checker\">Grammar checker</a></li>\\n<li><a href=\"/wiki/Predictive_text\" title=\"Predictive text\">Predictive text</a></li>\\n<li><a href=\"/wiki/Pronunciation_assessment\" title=\"Pronunciation assessment\">Pronunciation assessment</a></li>\\n<li><a href=\"/wiki/Spell_checker\" title=\"Spell checker\">Spell checker</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Natural-language_user_interface\" title=\"Natural-language user interface\">Natural language<br />user interface</a></th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Chatbot\" title=\"Chatbot\">Chatbot</a></li>\\n<li><a href=\"/wiki/Interactive_fiction\" title=\"Interactive fiction\">Interactive fiction</a> (c.f. <a href=\"/wiki/Syntax_guessing\" class=\"mw-redirect\" title=\"Syntax guessing\">Syntax guessing</a>)</li>\\n<li><a href=\"/wiki/Question_answering\" title=\"Question answering\">Question answering</a></li>\\n<li><a href=\"/wiki/Virtual_assistant\" title=\"Virtual assistant\">Virtual assistant</a></li>\\n<li><a href=\"/wiki/Voice_user_interface\" title=\"Voice user interface\">Voice user interface</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Related</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\\n<ul><li><a href=\"/wiki/Formal_semantics_(natural_language)\" title=\"Formal semantics (natural language)\">Formal semantics</a></li>\\n<li><a href=\"/wiki/Hallucination_(artificial_intelligence)\" title=\"Hallucination (artificial intelligence)\">Hallucination</a></li>\\n<li><a href=\"/wiki/Natural_Language_Toolkit\" title=\"Natural Language Toolkit\">Natural Language Toolkit</a></li>\\n<li><a href=\"/wiki/SpaCy\" title=\"SpaCy\">spaCy</a></li></ul>\\n</div></td></tr></tbody></table></div>\\n<!-- \\nNewPP limit report\\nParsed by mw\\xe2\\x80\\x90web.eqiad.main\\xe2\\x80\\x908669bc5c8\\xe2\\x80\\x908z889\\nCached time: 20250318155510\\nCache expiry: 2592000\\nReduced expiry: false\\nComplications: [vary\\xe2\\x80\\x90revision\\xe2\\x80\\x90sha1, show\\xe2\\x80\\x90toc]\\nCPU time usage: 1.014 seconds\\nReal time usage: 1.208 seconds\\nPreprocessor visited node count: 2980/1000000\\nPost\\xe2\\x80\\x90expand include size: 121971/2097152 bytes\\nTemplate argument size: 2007/2097152 bytes\\nHighest expansion depth: 12/100\\nExpensive parser function count: 6/500\\nUnstrip recursion depth: 1/20\\nUnstrip post\\xe2\\x80\\x90expand size: 146472/5000000 bytes\\nLua time usage: 0.722/10.000 seconds\\nLua memory usage: 8731889/52428800 bytes\\nNumber of Wikibase entities loaded: 0/400\\n-->\\n<!--\\nTransclusion expansion time report (%,ms,calls,template)\\n100.00% 1079.307      1 -total\\n 44.14%  476.395      1 Template:Reflist\\n 28.26%  305.036     21 Template:Cite_book\\n 14.90%  160.827      1 Template:Natural_Language_Processing\\n 14.73%  158.944      3 Template:Navbox\\n 10.30%  111.147      8 Template:Cite_journal\\n  9.35%  100.931      1 Template:Short_description\\n  7.30%   78.746      1 Template:More_citations_needed\\n  6.96%   75.130      2 Template:Ambox\\n  5.83%   62.871      1 Template:Sfn\\n-->\\n\\n<!-- Saved in parser cache with key enwiki:pcache:637199:|#|:idhash:canonical and timestamp 20250318155510 and revision id 1236297853. Rendering was triggered because: page-view\\n -->\\n</div><!--esi <esi:include src=\"/esitest-fa8a495983347898/content\" /> --><noscript><img src=\"https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&amp;type=1x1&amp;usesul3=0\" alt=\"\" width=\"1\" height=\"1\" style=\"border: none; position: absolute;\"></noscript>\\n<div class=\"printfooter\" data-nosnippet=\"\">Retrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;oldid=1236297853\">https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;oldid=1236297853</a>\"</div></div>\\n\\t\\t\\t\\t\\t<div id=\"catlinks\" class=\"catlinks\" data-mw=\"interface\"><div id=\"mw-normal-catlinks\" class=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Computational_linguistics\" title=\"Category:Computational linguistics\">Computational linguistics</a></li><li><a href=\"/wiki/Category:Natural_language_processing\" title=\"Category:Natural language processing\">Natural language processing</a></li><li><a href=\"/wiki/Category:Tasks_of_natural_language_processing\" title=\"Category:Tasks of natural language processing\">Tasks of natural language processing</a></li><li><a href=\"/wiki/Category:Data_mining\" title=\"Category:Data mining\">Data mining</a></li></ul></div><div id=\"mw-hidden-catlinks\" class=\"mw-hidden-catlinks mw-hidden-cats-hidden\">Hidden categories: <ul><li><a href=\"/wiki/Category:CS1_maint:_location_missing_publisher\" title=\"Category:CS1 maint: location missing publisher\">CS1 maint: location missing publisher</a></li><li><a href=\"/wiki/Category:CS1_maint:_archived_copy_as_title\" title=\"Category:CS1 maint: archived copy as title\">CS1 maint: archived copy as title</a></li><li><a href=\"/wiki/Category:CS1_maint:_bot:_original_URL_status_unknown\" title=\"Category:CS1 maint: bot: original URL status unknown\">CS1 maint: bot: original URL status unknown</a></li><li><a href=\"/wiki/Category:Webarchive_template_wayback_links\" title=\"Category:Webarchive template wayback links\">Webarchive template wayback links</a></li><li><a href=\"/wiki/Category:Articles_with_short_description\" title=\"Category:Articles with short description\">Articles with short description</a></li><li><a href=\"/wiki/Category:Short_description_is_different_from_Wikidata\" title=\"Category:Short description is different from Wikidata\">Short description is different from Wikidata</a></li><li><a href=\"/wiki/Category:Articles_needing_additional_references_from_April_2022\" title=\"Category:Articles needing additional references from April 2022\">Articles needing additional references from April 2022</a></li><li><a href=\"/wiki/Category:All_articles_needing_additional_references\" title=\"Category:All articles needing additional references\">All articles needing additional references</a></li><li><a href=\"/wiki/Category:All_accuracy_disputes\" title=\"Category:All accuracy disputes\">All accuracy disputes</a></li><li><a href=\"/wiki/Category:Articles_with_disputed_statements_from_June_2018\" title=\"Category:Articles with disputed statements from June 2018\">Articles with disputed statements from June 2018</a></li><li><a href=\"/wiki/Category:All_articles_with_unsourced_statements\" title=\"Category:All articles with unsourced statements\">All articles with unsourced statements</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_June_2018\" title=\"Category:Articles with unsourced statements from June 2018\">Articles with unsourced statements from June 2018</a></li><li><a href=\"/wiki/Category:Articles_to_be_expanded_from_February_2017\" title=\"Category:Articles to be expanded from February 2017\">Articles to be expanded from February 2017</a></li><li><a href=\"/wiki/Category:All_articles_to_be_expanded\" title=\"Category:All articles to be expanded\">All articles to be expanded</a></li><li><a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">CS1 maint: multiple names: authors list</a></li></ul></div></div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</main>\\n\\t\\t\\t\\n\\t\\t</div>\\n\\t\\t<div class=\"mw-footer-container\">\\n\\t\\t\\t\\n<footer id=\"footer\" class=\"mw-footer\" >\\n\\t<ul id=\"footer-info\">\\n\\t<li id=\"footer-info-lastmod\"> This page was last edited on 23 July 2024, at 23:13<span class=\"anonymous-show\">&#160;(UTC)</span>.</li>\\n\\t<li id=\"footer-info-copyright\">Text is available under the <a href=\"/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\" title=\"Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License\">Creative Commons Attribution-ShareAlike 4.0 License</a>;\\nadditional terms may apply. By using this site, you agree to the <a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use\" class=\"extiw\" title=\"foundation:Special:MyLanguage/Policy:Terms of Use\">Terms of Use</a> and <a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\" class=\"extiw\" title=\"foundation:Special:MyLanguage/Policy:Privacy policy\">Privacy Policy</a>. Wikipedia\\xc2\\xae is a registered trademark of the <a rel=\"nofollow\" class=\"external text\" href=\"https://wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\\n</ul>\\n\\n\\t<ul id=\"footer-places\">\\n\\t<li id=\"footer-places-privacy\"><a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\">Privacy policy</a></li>\\n\\t<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\">About Wikipedia</a></li>\\n\\t<li id=\"footer-places-disclaimers\"><a href=\"/wiki/Wikipedia:General_disclaimer\">Disclaimers</a></li>\\n\\t<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\\n\\t<li id=\"footer-places-wm-codeofconduct\"><a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct\">Code of Conduct</a></li>\\n\\t<li id=\"footer-places-developers\"><a href=\"https://developer.wikimedia.org\">Developers</a></li>\\n\\t<li id=\"footer-places-statslink\"><a href=\"https://stats.wikimedia.org/#/en.wikipedia.org\">Statistics</a></li>\\n\\t<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement\">Cookie statement</a></li>\\n\\t<li id=\"footer-places-mobileview\"><a href=\"//en.m.wikipedia.org/w/index.php?title=Automatic_summarization&amp;mobileaction=toggle_view_mobile\" class=\"noprint stopMobileRedirectToggle\">Mobile view</a></li>\\n</ul>\\n\\n\\t<ul id=\"footer-icons\" class=\"noprint\">\\n\\t<li id=\"footer-copyrightico\"><a href=\"https://wikimediafoundation.org/\" class=\"cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled\"><picture><source media=\"(min-width: 500px)\" srcset=\"/static/images/footer/wikimedia-button.svg\" width=\"84\" height=\"29\"><img src=\"/static/images/footer/wikimedia.svg\" width=\"25\" height=\"25\" alt=\"Wikimedia Foundation\" lang=\"en\" loading=\"lazy\"></picture></a></li>\\n\\t<li id=\"footer-poweredbyico\"><a href=\"https://www.mediawiki.org/\" class=\"cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled\"><picture><source media=\"(min-width: 500px)\" srcset=\"/w/resources/assets/poweredby_mediawiki.svg\" width=\"88\" height=\"31\"><img src=\"/w/resources/assets/mediawiki_compact.svg\" alt=\"Powered by MediaWiki\" lang=\"en\" width=\"25\" height=\"25\" loading=\"lazy\"></picture></a></li>\\n</ul>\\n\\n</footer>\\n\\n\\t\\t</div>\\n\\t</div> \\n</div> \\n<div class=\"vector-header-container vector-sticky-header-container\">\\n\\t<div id=\"vector-sticky-header\" class=\"vector-sticky-header\">\\n\\t\\t<div class=\"vector-sticky-header-start\">\\n\\t\\t\\t<div class=\"vector-sticky-header-icon-start vector-button-flush-left vector-button-flush-right\" aria-hidden=\"true\">\\n\\t\\t\\t\\t<button class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-sticky-header-search-toggle\" tabindex=\"-1\" data-event-name=\"ui.vector-sticky-search-form.icon\"><span class=\"vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search\"></span>\\n\\n<span>Search</span>\\n\\t\\t\\t</button>\\n\\t\\t</div>\\n\\t\\t\\t\\n\\t\\t<div role=\"search\" class=\"vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box\">\\n\\t\\t\\t<div class=\"vector-typeahead-search-container\">\\n\\t\\t\\t\\t<div class=\"cdx-typeahead-search cdx-typeahead-search--show-thumbnail\">\\n\\t\\t\\t\\t\\t<form action=\"/w/index.php\" id=\"vector-sticky-search-form\" class=\"cdx-search-input cdx-search-input--has-end-button\">\\n\\t\\t\\t\\t\\t\\t<div  class=\"cdx-search-input__input-wrapper\"  data-search-loc=\"header-moved\">\\n\\t\\t\\t\\t\\t\\t\\t<div class=\"cdx-text-input cdx-text-input--has-start-icon\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tclass=\"cdx-text-input__input\"\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\ttype=\"search\" name=\"search\" placeholder=\"Search Wikipedia\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<span class=\"cdx-text-input__icon cdx-text-input__start-icon\"></span>\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t<input type=\"hidden\" name=\"title\" value=\"Special:Search\">\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t<button class=\"cdx-button cdx-search-input__end-button\">Search</button>\\n\\t\\t\\t\\t\\t</form>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t<div class=\"vector-sticky-header-context-bar\">\\n\\t\\t\\t\\t<nav aria-label=\"Contents\" class=\"vector-toc-landmark\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<div id=\"vector-sticky-header-toc\" class=\"vector-dropdown mw-portlet mw-portlet-sticky-header-toc vector-sticky-header-toc vector-button-flush-left\"  >\\n\\t\\t\\t\\t\\t\\t<input type=\"checkbox\" id=\"vector-sticky-header-toc-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-sticky-header-toc\" class=\"vector-dropdown-checkbox \"  aria-label=\"Toggle the table of contents\"  >\\n\\t\\t\\t\\t\\t\\t<label id=\"vector-sticky-header-toc-label\" for=\"vector-sticky-header-toc-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet\"></span>\\n\\n<span class=\"vector-dropdown-label-text\">Toggle the table of contents</span>\\n\\t\\t\\t\\t\\t\\t</label>\\n\\t\\t\\t\\t\\t\\t<div class=\"vector-dropdown-content\">\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div id=\"vector-sticky-header-toc-unpinned-container\" class=\"vector-unpinned-container\">\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t</nav>\\n\\t\\t\\t\\t<div class=\"vector-sticky-header-context-bar-primary\" aria-hidden=\"true\" ><span class=\"mw-page-title-main\">Automatic summarization</span></div>\\n\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t<div class=\"vector-sticky-header-end\" aria-hidden=\"true\">\\n\\t\\t\\t<div class=\"vector-sticky-header-icons\">\\n\\t\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only\" id=\"ca-talk-sticky-header\" tabindex=\"-1\" data-event-name=\"talk-sticky-header\"><span class=\"vector-icon mw-ui-icon-speechBubbles mw-ui-icon-wikimedia-speechBubbles\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only\" id=\"ca-subject-sticky-header\" tabindex=\"-1\" data-event-name=\"subject-sticky-header\"><span class=\"vector-icon mw-ui-icon-article mw-ui-icon-wikimedia-article\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only\" id=\"ca-history-sticky-header\" tabindex=\"-1\" data-event-name=\"history-sticky-header\"><span class=\"vector-icon mw-ui-icon-wikimedia-history mw-ui-icon-wikimedia-wikimedia-history\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only mw-watchlink\" id=\"ca-watchstar-sticky-header\" tabindex=\"-1\" data-event-name=\"watch-sticky-header\"><span class=\"vector-icon mw-ui-icon-wikimedia-star mw-ui-icon-wikimedia-wikimedia-star\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only\" id=\"ca-edit-sticky-header\" tabindex=\"-1\" data-event-name=\"wikitext-edit-sticky-header\"><span class=\"vector-icon mw-ui-icon-wikimedia-wikiText mw-ui-icon-wikimedia-wikimedia-wikiText\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only\" id=\"ca-ve-edit-sticky-header\" tabindex=\"-1\" data-event-name=\"ve-edit-sticky-header\"><span class=\"vector-icon mw-ui-icon-wikimedia-edit mw-ui-icon-wikimedia-wikimedia-edit\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only\" id=\"ca-viewsource-sticky-header\" tabindex=\"-1\" data-event-name=\"ve-edit-protected-sticky-header\"><span class=\"vector-icon mw-ui-icon-wikimedia-editLock mw-ui-icon-wikimedia-wikimedia-editLock\"></span>\\n\\n<span></span>\\n\\t\\t\\t</a>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"vector-sticky-header-buttons\">\\n\\t\\t\\t\\t<button class=\"cdx-button cdx-button--weight-quiet mw-interlanguage-selector\" id=\"p-lang-btn-sticky-header\" tabindex=\"-1\" data-event-name=\"ui.dropdown-p-lang-btn-sticky-header\"><span class=\"vector-icon mw-ui-icon-wikimedia-language mw-ui-icon-wikimedia-wikimedia-language\"></span>\\n\\n<span>20 languages</span>\\n\\t\\t\\t</button>\\n\\t\\t\\t<a href=\"#\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive\" id=\"ca-addsection-sticky-header\" tabindex=\"-1\" data-event-name=\"addsection-sticky-header\"><span class=\"vector-icon mw-ui-icon-speechBubbleAdd-progressive mw-ui-icon-wikimedia-speechBubbleAdd-progressive\"></span>\\n\\n<span>Add topic</span>\\n\\t\\t\\t</a>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"vector-sticky-header-icon-end\">\\n\\t\\t\\t\\t<div class=\"vector-user-links\">\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t</div>\\n</div>\\n<div class=\"mw-portlet mw-portlet-dock-bottom emptyPortlet\" id=\"p-dock-bottom\">\\n\\t<ul>\\n\\t\\t\\n\\t</ul>\\n</div>\\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgHostname\":\"mw-web.eqiad.main-dd5c8cc7b-qrf69\",\"wgBackendResponseTime\":169,\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"1.014\",\"walltime\":\"1.208\",\"ppvisitednodes\":{\"value\":2980,\"limit\":1000000},\"postexpandincludesize\":{\"value\":121971,\"limit\":2097152},\"templateargumentsize\":{\"value\":2007,\"limit\":2097152},\"expansiondepth\":{\"value\":12,\"limit\":100},\"expensivefunctioncount\":{\"value\":6,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":146472,\"limit\":5000000},\"entityaccesscount\":{\"value\":0,\"limit\":400},\"timingprofile\":[\"100.00% 1079.307      1 -total\",\" 44.14%  476.395      1 Template:Reflist\",\" 28.26%  305.036     21 Template:Cite_book\",\" 14.90%  160.827      1 Template:Natural_Language_Processing\",\" 14.73%  158.944      3 Template:Navbox\",\" 10.30%  111.147      8 Template:Cite_journal\",\"  9.35%  100.931      1 Template:Short_description\",\"  7.30%   78.746      1 Template:More_citations_needed\",\"  6.96%   75.130      2 Template:Ambox\",\"  5.83%   62.871      1 Template:Sfn\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.722\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":8731889,\"limit\":52428800},\"limitreport-logs\":\"anchor_id_list = table#1 {\\\\n    [\\\\\"CITEREFAlrehamy2018\\\\\"] = 1,\\\\n    [\\\\\"CITEREFAlrehamyWalker2018\\\\\"] = 1,\\\\n    [\\\\\"CITEREFAndrew2007\\\\\"] = 1,\\\\n    [\\\\\"CITEREFAnne2004\\\\\"] = 1,\\\\n    [\\\\\"CITEREFAnnie2009\\\\\"] = 1,\\\\n    [\\\\\"CITEREFElena2009\\\\\"] = 1,\\\\n    [\\\\\"CITEREFElhamifarSapiroVidal2012\\\\\"] = 1,\\\\n    [\\\\\"CITEREFEndres-Niggemeyer1998\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHercules2003\\\\\"] = 1,\\\\n    [\\\\\"CITEREFHuff2010\\\\\"] = 1,\\\\n    [\\\\\"CITEREFLehmam2010\\\\\"] = 1,\\\\n    [\\\\\"CITEREFLi_TanYangqiu_SongShixia_LiuLexing_Xie2012\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMademlisTefasNikolaidisPitas2016\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMademlisTefasPitas2018\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMani2001\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMarcu2000\\\\\"] = 1,\\\\n    [\\\\\"CITEREFMiranda-Jim\\xc3\\xa9nez2013\\\\\"] = 1,\\\\n    [\\\\\"CITEREFPanTangDongMa2021\\\\\"] = 1,\\\\n    [\\\\\"CITEREFPotthastHagenStein2016\\\\\"] = 1,\\\\n    [\\\\\"CITEREFRoxana2002\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSankar_K._PalAlfredo_PetrosinoLucia_Maddalena2012\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSarkerMollaParis2013\\\\\"] = 1,\\\\n    [\\\\\"CITEREFSquire2016\\\\\"] = 1,\\\\n    [\\\\\"CITEREFTorres-Moreno,_Juan-Manuel2014\\\\\"] = 1,\\\\n    [\\\\\"CITEREFTurney2002\\\\\"] = 1,\\\\n    [\\\\\"CITEREFWidyassariRustadShidikNoersasongko2020\\\\\"] = 1,\\\\n    [\\\\\"CITEREFXiaojin2007\\\\\"] = 1,\\\\n    [\\\\\"CITEREFYatskoStarikovButakov2010\\\\\"] = 1,\\\\n    [\\\\\"CITEREFYatskoVishnyakov2007\\\\\"] = 1,\\\\n    [\\\\\"CITEREFZhai2016\\\\\"] = 1,\\\\n}\\\\ntemplate_list = table#1 {\\\\n    [\\\\\"Citation needed\\\\\"] = 1,\\\\n    [\\\\\"Cite book\\\\\"] = 21,\\\\n    [\\\\\"Cite conference\\\\\"] = 1,\\\\n    [\\\\\"Cite journal\\\\\"] = 8,\\\\n    [\\\\\"Cite news\\\\\"] = 1,\\\\n    [\\\\\"Cite web\\\\\"] = 6,\\\\n    [\\\\\"Doi\\\\\"] = 1,\\\\n    [\\\\\"Dubious\\\\\"] = 1,\\\\n    [\\\\\"Expand section\\\\\"] = 1,\\\\n    [\\\\\"Main\\\\\"] = 1,\\\\n    [\\\\\"More citations needed\\\\\"] = 1,\\\\n    [\\\\\"Natural Language Processing\\\\\"] = 1,\\\\n    [\\\\\"ProQuest\\\\\"] = 1,\\\\n    [\\\\\"Reflist\\\\\"] = 1,\\\\n    [\\\\\"Sfn\\\\\"] = 1,\\\\n    [\\\\\"Short description\\\\\"] = 1,\\\\n    [\\\\\"Webarchive\\\\\"] = 1,\\\\n}\\\\narticle_whitelist = table#1 {\\\\n}\\\\nciteref_patterns = table#1 {\\\\n}\\\\n\"},\"cachereport\":{\"origin\":\"mw-web.eqiad.main-8669bc5c8-8z889\",\"timestamp\":\"20250318155510\",\"ttl\":2592000,\"transientcontent\":false}}});});</script>\\n<script type=\"application/ld+json\">{\"@context\":\"https:\\\\/\\\\/schema.org\",\"@type\":\"Article\",\"name\":\"Automatic summarization\",\"url\":\"https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Automatic_summarization\",\"sameAs\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q1394144\",\"mainEntity\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q1394144\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\/\\\\/www.wikimedia.org\\\\/static\\\\/images\\\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2004-05-05T19:38:37Z\",\"dateModified\":\"2024-07-23T23:13:53Z\",\"headline\":\"computer-based method for shortening a text\"}</script>\\n</body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HtmlParser.from_string(content.decode('utf-8'), url, Tokenizer(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize - TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TextRankSummarizer()\n",
    "summary = summarizer(parser.document, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
      "Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n"
     ]
    }
   ],
   "source": [
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different Summarizers\n",
    "- LexRankSummarizer\n",
    "- LuhnSummarizer\n",
    "- LsaSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_summarizer = LexRankSummarizer()\n",
    "lhun_summarrizer = LuhnSummarizer()\n",
    "lsa_summarizer = LsaSummarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.\n",
      "For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.\n",
      "Automatic Text Summarization.\n"
     ]
    }
   ],
   "source": [
    "summary = lex_summarizer(parser.document, 5) \n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LuhnSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\n",
      "It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n"
     ]
    }
   ],
   "source": [
    "summary = lhun_summarrizer(parser.document, 5) \n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\n",
      "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n",
      "It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n"
     ]
    }
   ],
   "source": [
    "summary = lsa_summarizer(parser.document, 5) \n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Take a piece of text from wiki page and summarize them using Gensim\n",
    "### Steps\n",
    "- Install the necessary libraries\n",
    "- Import the libraries\n",
    "- Scrape the text from a pre-defined webpage\n",
    "- Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the text\n",
    "- Use beautifulSoup to extract text (from Task1 of ML-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\\n Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\\n In 2022 Google Docs released an automatic summarization feature.[9]\\n There are two general approaches to automatic summarization: extraction and abstraction.\\n Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\\n Abstractive summarization methods generate new text that did not exist in the original text.[12] This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to images and videos, which is why most summarization systems are extractive.\\n Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\\n There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\n An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\\n Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[13] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\\n At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\\n The task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.[14] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\\nConsider the example text from a news article:\\n A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep understanding of the text, which makes it difficult for a computer system.\\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\\n Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.\\n Beginning with the work of Turney,[15] many researchers have approached keyphrase extraction as a supervised machine learning problem.\\nGiven a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\\nKeyphrase extractors are generally evaluated using precision and recall. Precision measures how\\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\\nharmonic mean of the two (F\\xa0=\\xa02PR/(P\\xa0+\\xa0R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\\n Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\\n We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney\\'s seminal paper.\\n In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\\n Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. In the case of Turney\\'s GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\\n Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney\\'s results demonstrate.\\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text\\'s intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\\n TextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\\n The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\\n Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\\n Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\\n It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\\n In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\\n Like keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.\\n Supervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 evaluation only considers unigrams.\\n During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a Naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\\n A promising approach is adaptive document/text summarization.[17] It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre. Such software has been created.[18]\\n The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\\n A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank[19] is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\\n In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\\n The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences\\' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights.\\n In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\\n It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\\n Unlike TextRank, LexRank has been applied to multi-document summarization.\\n Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. Multi-document summarization may also be done in response to a question.[20][11]\\n Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. [dubious – discuss]\\n Multi-document extractive summarization faces a problem of redundancy. Ideally, we want to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). For example, in a set of news articles about some event, each article is likely to have many similar sentences. To address this issue, LexRank applies a heuristic post-processing step that adds sentences in rank order, but discards sentences that are too similar to ones already in the summary. This method is called Cross-Sentence Information Subsumption (CSIS). These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. Its importance also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to an arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\\n A related method is Maximal Marginal Relevance (MMR),[21] which uses a general-purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks (a random walk where certain states end the walk). The algorithm is called GRASSHOPPER.[22] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\\n The state of the art results for multi-document summarization are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\\n A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position. It does not use word frequency, training or preprocessing. It uses two user-supplied parameters: equivalence (when are two sentences to be considered equivalent?) and relevance (how long is the desired summary?).\\n The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\\n While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee.[25] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\\n Submodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012[26] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,[27] shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.[citation needed]\\n Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show[28] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015[29] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.[30]\\n Specific applications of automatic summarization include:\\n The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\\n Evaluation can be intrinsic or extrinsic,[36] and inter-textual or intra-textual.[37]\\n Intrinsic evaluation assesses the summaries directly, while extrinsic evaluation evaluates how the summarization system affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\\n Intra-textual evaluation assess the output of a specific summarization system, while inter-textual evaluation focuses on contrastive analysis of outputs of several summarization systems.\\n Human judgement often varies greatly in what it considers a \"good\" summary, so creating an automatic evaluation process is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive, as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.\\n The most common way to evaluate summaries is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). It is very common for summarization and translation systems in NIST\\'s Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references. It calculates n-gram overlaps between automatically generated summaries and previously written human summaries. It is recall-based to encourage inclusion of all important topics in summaries. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is the fraction of unigrams that appear in both the reference summary and the automatic summary out of all unigrams in the reference summary. If there are multiple reference summaries, their scores are averaged. A high level of overlap should indicate a high degree of shared concepts between the two summaries.\\n ROUGE cannot determine if the result is coherent, that is if sentences flow together in a sensibly. High-order n-gram ROUGE measures help to some degree.\\n Another unsolved problem is Anaphor resolution. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.[38]\\n Domain-independent summarization techniques apply sets of general features to identify information-rich text segments. Recent research focuses on domain-specific summarization using knowledge specific to the text\\'s domain, such as medical knowledge and ontologies for summarizing medical texts.[39]\\n The main drawback of the evaluation systems so far is that we need a reference summary (for some methods, more than one), to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be made to create corpora of texts and their corresponding summaries. Furthermore, some methods require manual annotation of the summaries (e.g. SCU in the Pyramid Method). Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\\n The first publication in the area dates back to 1957 [40] (Hans Peter Luhn), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020, the field was still very active and research is shifting towards abstractive summation and real-time summarization.[41]\\n Recently the rise of transformer models replacing more traditional RNN (LSTM) have provided a flexibility in the mapping of text sequences to text sequences of a different type, which is well suited to automatic summarization. This includes models such as T5[42] and Pegasus.[43]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = collect_text(get_page(url))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize\n",
    "- **word_count**: maximum amount of words we want in the summary\n",
    "- **ratio**: fraction of sentences in the original text should be returned as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary by word count:\n",
      "Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
      "\n",
      "Summary by ratio:\n",
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
      "Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms.\n",
      "Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
      "Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\n",
      "Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express.\n",
      "Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n",
      "Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n",
      "For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\n",
      "The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n",
      "Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.\n",
      "These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\n"
     ]
    }
   ],
   "source": [
    "# Summarize the text using word_count\n",
    "summary_by_word_count = summarize(text, word_count=100)  # Summarize the document to 100 words\n",
    "\n",
    "# Summarize the text using ratio\n",
    "summary_by_ratio = summarize(text, ratio=0.05)  # Summarize the document to 5% of the original length\n",
    "\n",
    "print(\"Summary by word count:\")\n",
    "print(summary_by_word_count)\n",
    "print(\"\\nSummary by ratio:\")\n",
    "print(summary_by_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Take a piece of text from wiki page and summarize them using Gensim\n",
    "### Steps\n",
    "- Install the necessary libraries\n",
    "- Import the libraries\n",
    "- Scrape the text from a pre-defined webpage\n",
    "- Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the text\n",
    "- Use beautifulSoup to extract text (from Task1 of ML-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary by word count:\n",
      "Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms.\n",
      "Like keyphrase extraction, document summarization aims to identify the essence of a text.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "\n",
      "Summary by ratio:\n",
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
      "Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms.\n",
      "Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\n",
      "Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n",
      "Like keyphrase extraction, document summarization aims to identify the essence of a text.\n",
      "Supervised text summarization is very much like supervised keyphrase extraction.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.\n",
      "These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\n",
      "For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.\n",
      "It is very common for summarization and translation systems in NIST's Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the text using word_count\n",
    "summary_by_word_count = summarizer.summarize(text, words=100)  # Summarize the document to 100 words\n",
    "\n",
    "# Summarize the text using ratio\n",
    "summary_by_ratio = summarizer.summarize(text, ratio=0.05)  # Summarize the document to 5% of the original length\n",
    "\n",
    "print(\"Summary by word count:\")\n",
    "print(summary_by_word_count)\n",
    "print(\"\\nSummary by ratio:\")\n",
    "print(summary_by_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT: Take the same medium article (the one I wrote) we used for Task 1 of ML-1 and extract the text and summarize them using all the above methods and provide the best summary with a note saying why the chosen library is the best\n",
    "url = https://medium.com/@subashgandyer/papa-what-is-a-neural-network-c5e5cc427c7\n",
    "\n",
    "### Submit 2 files\n",
    "- (notebook) .ipynb\n",
    "- (summary) .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim==3.8.3 scipy==1.5.4 summa sumy beautifulsoup4 requests lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.summarization import summarize as gensim_summarize\n",
    "from summa import summarizer as summa_summarize\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sign up Sign in Sign up Sign in Home Library Stories Stats Subash Gandyer Follow -- 1 Listen Share It was a cozy Sunday afternoon in the month of February 2018. I just finished my huge customary Sunday lunch spread with family and resting along. Everyone in the family was taking a quick nap for a pre-planned evening outing. Well not everyone, actually. My 4-year-old angel came running to me, asked me to play with her for a while. As I was lazy and not in a position to move after the big spread, I evaded the chance to play with her by telling her “Papa’s got some work baby. Got to code some stuff.” I thought that would be the end of the conversation. No! It wasn’t. As my daughter was very inquisitive, she asked me “Papa, what stuff?” I said, “I need to code something for my work.” She didn’t leave. She again asked, “What is code something?” I wanted to end this conversation, as I was half past asleep. “Just some stuff baby. You wouldn’t understand. Way beyond your age.” Tanishi never takes NO for an answer. “Papa, tell me what stuff means and something means.” Cannot help evade a cute curious face, I said, “I am working on Neural Network.” Before I finish the statement, “Papa, What is a Meural Metark?” I gave up my stubbornness of avoiding her. With a smile, I said slowly, “Its Neu — ral Net — work” She asked, “Papa, What is Meu-ral Met-ark?” At the back of my head, thoughts of me taking days to comprehend what a NN (short for Neural Network) is, how it would work, where it is used, how it is simulating our human brain’s inner workings were going through. It took me days and weeks to understand fully and how can I make a 4-year-old understand what a Neural Network is. I never thought about teaching a complex concept to a kid before. It was way beyond their age. It was not a mere color or a shape or a number to teach. This was going to be interesting and challenging. I saw an opportunity here as a teacher (teaching professional college adults) to simplify the complex problem to a simpler understandable concept for a kid who doesn’t know anything apart from ABC, 1 to 20, Colors and Shapes. WOW! This was mind-boggling. I gave a couple of lectures to make my adult students understand what a Neural Network is when I taught them a mini-Machine Learning course. I rolled up my sleeves. Took up the challenge. Sat down beside my kid with pencil, drawing sheet and my laptop. Thinking of ways of what would be the best way to start teaching her. One simple methodology: I thought of starting with an easy-to-follow definition. “Neural Network is a collection (a network) of neurons whose job is to learn a new thing or a new place or a new process or a new concept.” It would be stupid on my part to start with a definition of Neural Network like how we used to teach adults in college. No, it’s not right for a kid. Then, I thought of a way to start with her favorite hobby of drawing and coloring objects. Gave her a paper, ask her to draw a circle. She did. She didn’t stop there. She drew a face inside the circle. She did with great pride and smile. I drew a circle below. Asked her to draw a dog out of it. She drew a face on the circle, and then she drew one big ellipse next to head. Beneath the ellipse, she drew four legs downwards. She said, “That’s the dog daddy.” I was super impressed as her daddy. I said, “Good Job!” and asked her, “Where’s the tail, baby?” She smiled and drew a tail. That’s it. There’s a dog. I knew she never learnt to draw a lion. I took this opportunity to make her learn a new thing. After all, neural network inside our brain helps us to learn new things in our life. I said, “Baby, we would learn how to draw a lion now.” She’s bustling with energy said “Yayyyy! Lion!” and made “Grrrrrrrrr!!!” I tested her by asking her to try drawing a lion. She stumbled. She never came across a lion before. I told her, there would be a circle face, ellipse body, four legs, a tail and so on. What I was actually doing here was teaching her neural network (brain) the features of a lion like exactly how Machine Learning Engineers would train the machine to learn new features. After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before. I said, “Very good”. But they look similar right? Now, we need to know the difference between a dog and a lion. What’s the difference? Lion is big. Dog is small. One big difference is the mane (the beard) just like your papa has. A big beard is the main difference between a lion and a dog. Now I took the pencil and drew a beard in the face. I told her this is a lion. A dog will have features like face, body, legs, and tail. A lion will have features like face, body, legs, tail and a beard. She was little confused, rightly so. Tried a different approach to teach her with visuals. I spend some time in collecting pictures of dogs and lions from Google images. Printed it and showed her a set of dogs and then a set of lions. I asked her what these pictures look like to you? Was it a dog or a lion? She kept mixing the answers first. After rewarding her for correct classifications with nice adjectives and correcting her for wrong classifications, her detection accuracy improved a lot after sometime. She slowly got used to it. She got trained. Her neural network got aligned with classifying Dogs and Lions after some training. If you’ve noticed, this is how ML people make their machines learn through Reinforcement Learning. This was how I trained my daughter to classify dogs and lions. She was so happy that she learnt a new thing by doing her favorite pastime of drawing and coloring. She was happy and there was the neural network functioning concept that still remained to be taught to her. I asked her “What happened now baby? Did you learn something new today? Do you know what is the difference between a lion and a dog?” She said, “Yes.” I said, “This is called Learning. You just learnt a new thing today. How you learnt it is because of Neural Network inside your brain.” Now, a neural network is a collection of neurons that keeps switching on and off based on things you see, feel, hear and think just like switching on light bulb at our home. “Your brain is here inside our head. Your brain has billions of neurons inside. Every neuron has a purpose of seeing a feature. You can think of a feature as a simpler form of an object. For example, a face object can be seen by neurons as a circle (face) with two smaller circles (two eyes), a line (nose) and a curved line (mouth). These are features in a face object. Neurons will look for these patterns (circle, line, curved line, smaller circles and so on). Every neuron is waiting for your eyes to see something new, for your nose to smell something new, for your ears to hear something new, for your tongue to taste something new. When something new is heard, or smelled, or seen, or tasted, the neurons will group together to send signals and forms connections with already seen, heard, tasted or smelled neurons. This is what is called as ‘Forming Logical Connections’ with the past. Once it is established, your brain will say, ‘Hey, this is dog. This is lion and so on.’ Every neuron would’ve seen, or heard, or smelled, or tasted certain features. When you see a new object, your brain will ask the neurons, ‘Hey, anybody experienced this before?’ The neurons will say, ‘Yes, I have seen this.’ Certain other neurons will say, ‘No, I have not seen this.’ The neurons that have seen this before, will group together and form logical connections from the past and gives us an object from our memory. For example, when I showed you a lion picture, your brain asked the neurons who had seen it before. The neurons grouped together with features like face, body, legs, tail and a beard forms a lion. Your brain looks for these features. Once all the features are there, the neurons will send a signal that the picture you are looking at is a lion and not a dog. This is what happens inside your brain, darling. This is also how all our learning happens.” “In short, brain consists of billions of neurons. Every neuron will tune itself to pick up certain features like legs, tail, face, beard, and so on. When a picture is shown to you, your neurons will group together and tries to signal what that object is by forming logical connections between the past and the present. Ultimately, the neurons in your brain tell that it is a lion and not a dog. This is how your brain identifies things. This complex working of the neurons inside the brain works super fast in the order of milliseconds (very fast). In a few milliseconds, your brain identifies whether the picture is a lion or a dog. This is what a neural network is and this is how it works in identifying things. The same principle is applied for a song that you hear, a cartoon that you watch, a rhyme that you sing, an animal that you draw, a food that you taste, a flower that you smell and so on. Neurons work together in finding patterns and once a pattern is identified, it signals the brain that it is a thing, place, song, taste, smell and so on.” It was the time to test her understanding. I asked her simple questions to reiterate what she learnt just now. “Do you understand now, baby? Shall I ask you some questions? If you answer it correctly, I will take you out to the ice-cream parlor and you can have how much ever you want. “ The conversation went like this. Me: Where is your brain? Tanishi: Its here (showing her head) Me: Correct! Where are the neurons located? Tanishi: Here (showing her head) Me: Good! Who is responsible for identifying things, objects inside our brain? Tanishi: Hmmmm. Don’t know. Me: I will give a clue. It starts with ’N’ and ends with a rhyming sound of ‘ons’ Tanishi: Neurons Me: Good job, baby. Every neuron looks for something. What is a neuron looking for? I will give you a hint. Is it features? Tanishi: Yes. Me: Tell me what neurons are looking for? Tanishi: Features. Me: Great! When I ask you to draw a dog, what are the features there? Tanishi: Help me. I don’t know. Me: Okay, when I ask you to draw a dog, what is the first thing you draw? Tanishi: Circle. Me: Good. Circle represents what? Tanishi: Face Me: Yeah, baby. Then what you draw next? Tanishi: I draw eyes, nose, and mouth. Me: Good. That’s right. So, the face has got eyes, nose and mouth as features. Now tell me, what are the features in a face? Tanishi: Eyes, nose, mouth Me: Brilliant! After the face, what do you draw for completing a dog? Tanishi: Body Me: What shape is the body? Tanishi: Ellipse Me: Nice. This is also a feature that our neurons are looking for. What’s next? Tanishi: Legs Me: How many legs does a dog have? Tanishi: 1,2,3,4. 4 legs Me: Good. Anything else to draw in a dog? Tanishi: That’s it. Me: Hey how about a tail? Dogs have tail? Tanishi: Yes. Me: So, for a dog, the features are face, body, legs and tail. Do you understand what are the features now for a dog? Tanishi: Yes!!! Me: What are they? Tanishi: Face, body, legs and tail Me: Amazing baby. If I ask you to draw a lion, what are the features? Same features as a dog but with an extra feature. Can you tell me what that extra feature is for a lion? It starts with ‘B’ Tanishi: Beard. (Holding my beard, she smiled and said) Beard like you Me: Yes, darling. Good job. So, neurons are inside our brain waiting for features. To identify an object, it looks for features. Once the features are seen and a logical connection is established, neurons signals your brain that it is a lion. Who sends signal to the brain? Tanishi: Neurons Me: Now, these neurons are working together as friends. They are like your friends Daisy, Rose, Isabelle and Hayami. All neurons work together like your friends and identify lion and dog. So, this group of neuron friends is called as Neural Network. Tell me, what is a neural network? Tanishi: Neuron friends Me: Good. Neural network is a group of neuron friends identifying lions and dogs. Do you understand Neural Network now? Tanishi: Yes papa. Me: Brilliant job baby. Come, let’s go, and get some ice creams for my lovely angel. Phew! That was a challenging and a learning experience for me as well. That was how I ended my Sunday afternoon with my brilliant daughter who can teach other kids what a Meural Metark is. And I hope she will not come to me running asking “Papa, what is Meural Metark?” again. And I have a strong feeling; she would ask me another stunning question sooner or later. Fingers crossed and I am prepared to take up the challenges coming my way in the future. -- -- 1 CS Teacher to ... Help Status About Careers Press Blog Privacy Terms Text to speech Teams'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://medium.com/@subashgandyer/papa-what-is-a-neural-network-c5e5cc427c7\"\n",
    "text = collect_text(get_page(url))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary by Gensim (word count):\n",
      "What I was actually doing here was teaching her neural network (brain) the features of a lion like exactly how Machine Learning Engineers would train the machine to learn new features.\n",
      "After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before.\n",
      "The neurons grouped together with features like face, body, legs, tail and a beard forms a lion.\n",
      "Once all the features are there, the neurons will send a signal that the picture you are looking at is a lion and not a dog.\n",
      "\n",
      "Summary by Gensim (ratio):\n",
      "“Neural Network is a collection (a network) of neurons whose job is to learn a new thing or a new place or a new process or a new concept.” It would be stupid on my part to start with a definition of Neural Network like how we used to teach adults in college.\n",
      "What I was actually doing here was teaching her neural network (brain) the features of a lion like exactly how Machine Learning Engineers would train the machine to learn new features.\n",
      "After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before.\n",
      "A dog will have features like face, body, legs, and tail.\n",
      "A lion will have features like face, body, legs, tail and a beard.\n",
      "The neurons grouped together with features like face, body, legs, tail and a beard forms a lion.\n",
      "Once all the features are there, the neurons will send a signal that the picture you are looking at is a lion and not a dog.\n",
      "Tanishi: Yes. Me: So, for a dog, the features are face, body, legs and tail.\n",
      "Neural network is a group of neuron friends identifying lions and dogs.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the text using Gensim\n",
    "gensim_summary_word_count = gensim_summarize(text, word_count=100) \n",
    "gensim_summary_ratio = gensim_summarize(text, ratio=0.05) \n",
    "\n",
    "print(\"Summary by Gensim (word count):\")\n",
    "print(gensim_summary_word_count)\n",
    "print(\"\\nSummary by Gensim (ratio):\")\n",
    "print(gensim_summary_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary by Summa (word count):\n",
      "What I was actually doing here was teaching her neural network (brain) the features of a lion like exactly how Machine Learning Engineers would train the machine to learn new features.\n",
      "After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before.\n",
      "The neurons grouped together with features like face, body, legs, tail and a beard forms a lion.\n",
      "Tanishi: Yes. Me: So, for a dog, the features are face, body, legs and tail.\n",
      "\n",
      "Summary by Summa (ratio):\n",
      "“Neural Network is a collection (a network) of neurons whose job is to learn a new thing or a new place or a new process or a new concept.” It would be stupid on my part to start with a definition of Neural Network like how we used to teach adults in college.\n",
      "What I was actually doing here was teaching her neural network (brain) the features of a lion like exactly how Machine Learning Engineers would train the machine to learn new features.\n",
      "After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before.\n",
      "The neurons grouped together with features like face, body, legs, tail and a beard forms a lion.\n",
      "Once all the features are there, the neurons will send a signal that the picture you are looking at is a lion and not a dog.\n",
      "Ultimately, the neurons in your brain tell that it is a lion and not a dog.\n",
      "Tanishi: Yes. Me: So, for a dog, the features are face, body, legs and tail.\n",
      "All neurons work together like your friends and identify lion and dog.\n",
      "Neural network is a group of neuron friends identifying lions and dogs.\n"
     ]
    }
   ],
   "source": [
    "# Summarize the text using Summa\n",
    "summa_summary_word_count = summa_summarize.summarize(text, words=100) \n",
    "summa_summary_ratio = summa_summarize.summarize(text, ratio=0.05)  \n",
    "\n",
    "print(\"\\nSummary by Summa (word count):\")\n",
    "print(summa_summary_word_count)\n",
    "print(\"\\nSummary by Summa (ratio):\")\n",
    "print(summa_summary_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the text using Sumy\n",
    "parser = HtmlParser.from_string(text, url, Tokenizer(\"english\"))\n",
    "summarizers = {\n",
    "    \"TextRank\": TextRankSummarizer(),\n",
    "    \"LexRank\": LexRankSummarizer(),\n",
    "    \"Luhn\": LuhnSummarizer(),\n",
    "    \"Lsa\": LsaSummarizer()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_sumy = {}\n",
    "for name, summarizer in summarizers.items():\n",
    "    summary = summarizer(parser.document, 9)  # Summarize the document into 5 sentences\n",
    "    summaries_sumy[name] = '\\n'.join([str(sentence) for sentence in summary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary by Sumy (TextRank):\n",
      "“Papa, tell me what stuff means and something means.” Cannot help evade a cute curious face, I said, “I am working on Neural Network.” Before I finish the statement, “Papa, What is a Meural Metark?” I gave up my stubbornness of avoiding her.\n",
      "With a smile, I said slowly, “Its Neu — ral Net — work” She asked, “Papa, What is Meu-ral Met-ark?” At the back of my head, thoughts of me taking days to comprehend what a NN (short for Neural Network) is, how it would work, where it is used, how it is simulating our human brain’s inner workings were going through.\n",
      "“Neural Network is a collection (a network) of neurons whose job is to learn a new thing or a new place or a new process or a new concept.” It would be stupid on my part to start with a definition of Neural Network like how we used to teach adults in college.\n",
      "After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before.\n",
      "How you learnt it is because of Neural Network inside your brain.” Now, a neural network is a collection of neurons that keeps switching on and off based on things you see, feel, hear and think just like switching on light bulb at our home.\n",
      "When you see a new object, your brain will ask the neurons, ‘Hey, anybody experienced this before?’ The neurons will say, ‘Yes, I have seen this.’ Certain other neurons will say, ‘No, I have not seen this.’ The neurons that have seen this before, will group together and form logical connections from the past and gives us an object from our memory.\n",
      "Once all the features are there, the neurons will send a signal that the picture you are looking at is a lion and not a dog.\n",
      "The same principle is applied for a song that you hear, a cartoon that you watch, a rhyme that you sing, an animal that you draw, a food that you taste, a flower that you smell and so on.\n",
      "Neurons work together in finding patterns and once a pattern is identified, it signals the brain that it is a thing, place, song, taste, smell and so on.” It was the time to test her understanding.\n",
      "\n",
      "Summary by Sumy (LexRank):\n",
      "As my daughter was very inquisitive, she asked me “Papa, what stuff?” I said, “I need to code something for my work.” She didn’t leave.\n",
      "“Neural Network is a collection (a network) of neurons whose job is to learn a new thing or a new place or a new process or a new concept.” It would be stupid on my part to start with a definition of Neural Network like how we used to teach adults in college.\n",
      "I said, “Good Job!” and asked her, “Where’s the tail, baby?” She smiled and drew a tail.\n",
      "After telling her the features of a lion, asked her “Can you draw these for me?” She happily drew almost a similar figure to that of a dog she drew before.\n",
      "Was it a dog or a lion?\n",
      "Do you know what is the difference between a lion and a dog?” She said, “Yes.” I said, “This is called Learning.\n",
      "Ultimately, the neurons in your brain tell that it is a lion and not a dog.\n",
      "Tanishi: That’s it.\n",
      "Me: So, for a dog, the features are face, body, legs and tail.\n",
      "\n",
      "Summary by Sumy (Luhn):\n",
      "“Papa, tell me what stuff means and something means.” Cannot help evade a cute curious face, I said, “I am working on Neural Network.” Before I finish the statement, “Papa, What is a Meural Metark?” I gave up my stubbornness of avoiding her.\n",
      "With a smile, I said slowly, “Its Neu — ral Net — work” She asked, “Papa, What is Meu-ral Met-ark?” At the back of my head, thoughts of me taking days to comprehend what a NN (short for Neural Network) is, how it would work, where it is used, how it is simulating our human brain’s inner workings were going through.\n",
      "“Neural Network is a collection (a network) of neurons whose job is to learn a new thing or a new place or a new process or a new concept.” It would be stupid on my part to start with a definition of Neural Network like how we used to teach adults in college.\n",
      "How you learnt it is because of Neural Network inside your brain.” Now, a neural network is a collection of neurons that keeps switching on and off based on things you see, feel, hear and think just like switching on light bulb at our home.\n",
      "For example, a face object can be seen by neurons as a circle (face) with two smaller circles (two eyes), a line (nose) and a curved line (mouth).\n",
      "Every neuron is waiting for your eyes to see something new, for your nose to smell something new, for your ears to hear something new, for your tongue to taste something new.\n",
      "When something new is heard, or smelled, or seen, or tasted, the neurons will group together to send signals and forms connections with already seen, heard, tasted or smelled neurons.\n",
      "When you see a new object, your brain will ask the neurons, ‘Hey, anybody experienced this before?’ The neurons will say, ‘Yes, I have seen this.’ Certain other neurons will say, ‘No, I have not seen this.’ The neurons that have seen this before, will group together and form logical connections from the past and gives us an object from our memory.\n",
      "Neurons work together in finding patterns and once a pattern is identified, it signals the brain that it is a thing, place, song, taste, smell and so on.” It was the time to test her understanding.\n",
      "\n",
      "Summary by Sumy (Lsa):\n",
      "I just finished my huge customary Sunday lunch spread with family and resting along.\n",
      "As I was lazy and not in a position to move after the big spread, I evaded the chance to play with her by telling her “Papa’s got some work baby.\n",
      "What I was actually doing here was teaching her neural network (brain) the features of a lion like exactly how Machine Learning Engineers would train the machine to learn new features.\n",
      "I spend some time in collecting pictures of dogs and lions from Google images.\n",
      "If you’ve noticed, this is how ML people make their machines learn through Reinforcement Learning.\n",
      "For example, when I showed you a lion picture, your brain asked the neurons who had seen it before.\n",
      "Every neuron will tune itself to pick up certain features like legs, tail, face, beard, and so on.\n",
      "And I hope she will not come to me running asking “Papa, what is Meural Metark?” again.\n",
      "And I have a strong feeling; she would ask me another stunning question sooner or later.\n"
     ]
    }
   ],
   "source": [
    "for name, summary in summaries_sumy.items():\n",
    "    print(f\"\\nSummary by Sumy ({name}):\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
